# Цель данной ветки

* Перед merge в main добавить pandas в requirements или убрать его из кода

0) Убедился, что predict + aggregate выдает такой же файл submission.
0.5) Начал разрабатывать создание и заполнение таблицы в конце playground.ipynb

1) Добавить сохранение таблицы в predict.py -> save_results
      * главной проблемой является, что word_generator_kwargs, переданный скрипту может быть неисчерпывающим (если у алгоритма есть параметры по умолчанию) или наоборот избыточным (например, 'verbose': false). Обе стуации могут привести к проблемам: в случае избыточности два одинаковых predictor'а будут сочтены за разные, в случае недостаточности, наоборот, два разных predictor'а могут быть сочтены за один. Речь идет об этапе поиска predictor'а по уникальному сочитанию 'model_name', 'model_weights', 'generator_name', 'grid_name', *[f'gen_name__{k}' for k in generator_kwargs.keys()] c wелью дальнейшего заполнения пути до произведенного предсказания
      * Я вижу ровно 1 решение данной проблемы: в predict.py определить словарь word_generator_name__to__kwarg_keys: Dict[str, Set[str]], отображающий название алгоритма декодирования в множество ключей переданного kwargs. На основе этого словаря необходимо определить пред-условие для выполнения скрипта: `assert word_generator_name__to__kwarg_keys[word_generator_name] == set(config['word_generator_kwargs'])`
2) Убедиться, что предсказание + аггрегация выдают тот же результат, что и submission
3) Смерджить в ветку remake_after_finals, удалить ветку preditor_class
4) Хочется выделить из датасета в отдельный класс NearestKeyLookup. Во-первых, так можно будет сохранить состояние объекта этого класса и не ждать минуту каждый раз, когда грузится класс датасета. Во-вторых, на инференсе тоже нужно находить по координате ближайшую клавишу

* Наладить обучение с collate_fn

* Добавить в обучение логирование tensorboard

* может быть, нужен pandas MultiIndex

* Наладить связь между predict.py и aggregate_predictions.py
      * predict.py должен возвращать таблицу с predictior_id, Generator_type, Generator_call_kwargs_json, Model_architecture_name, Model_weights_path, Grid_name, test_preds_path, val_preds_path, validation_metric

      * В таблице хранить generator_kwargs как отдельные столбцы!! формируется так: f"{generator_name}_{kwarg_name}" -> kwarg_val

      * aggregation.py должен иметь аггрегаторы, каждый из которых имеет fit и predict, а также хранит всю информацию о predictor'ах. Fit производится на valid части, predict - на test части. Init агрегатора получает на вход таблицу и нужные redictor_id (по умолчанию все id) 

      * Возможно, Нужен utility, объединяющий несколько таблиц, если предсказания делались на разных машинах. Predicotr_id должен удаляться, далее обходим все уникальные сочетания (Generator_type, Generator_call_kwargs_json, Model_architecture_name, Model_weights_path, Grid_name). сочетание представлено более чем одной строчкиой, убеждаемся, что для каждого из столбцов {test_preds_path, val_preds_path, validation_metric} мощность объединения значений по этим строчкам не превышает 1. Если это так производим объединение и записываем в одну из строчек, остальные удаляем. Иначе вызываем ошибку. В конце переназначаем id и перезаписываем файл.




* Можно вообще не иметь id. Организовать базу данных либо в виде pandas таблички, либо такого словаря: tuple(model name, model path, grid_name, generator-name, tuple(sorted(kwargs.items()))) -> all_results_dict = {path to validation results, path to test results}




# Отличие от main на 20.02.24
* Datasetv3 вместо Datasetv2. Новая версия не возвращает traj_pad_mask и не производит padding траекторий. Padding будет производиться в collarte function.
* Prediction переписан. Используется predictor class, получабщий датасет и возвращающий объект с предсказаниями и всеми метаданными (model_name, generator_name, гиперпараметры и все, что нужно)
* Word generators переписаны, чтобы подходить Datasetv3
* Обучение (kaggle_notebook.ipynb) переписано (но не помню переписано ли до конца). Отличие опять же в использовании Datasetv3 и collate_fn


# General

Должен ли датасет получать на вход токенизатор (используется только в get_item).
Может быть, лучше чтобы он возвращал строку?
Также _nearest_kb_label_dict лучше создать 1 раз, сохранить в файл и передавать в уонструктор


Обученная модель = 
* Имя раскладки
* Путь до весов
* Имя архитектуры

Нужно ли делить json на отдельные grid'ы?

Если я уж разделил все датасеты на отдельные grid'ы можно валидационной и тестовой выборке добавить отдельный файл original indexes, чтобы удобно производить валидацию и удобно делать submission.

Думаю, создание словаря [координаты → лейбл ближайшей буквы] нужно вынести в отдельный скрипт и сделать единожды.
Возможно, лучше это будет словарь [координаты → список расстояний до каждой из клавиш ]. Мб это поможет сделать линейную интерполяцию.

# Prediction

При предсказании предлагается заполнять таблицу prediction_table.csv:
* word_generator_id aka predictor_id
      * (определяется всем, кроме test_preds_path, val_preds_path, validation_metric)
* –
* Generator_type
* Generator_call_kwargs_json
* Model_architecture_name
* Model_weights_path
* –
* Grid_name
* –
* test_preds_path
* val_preds_path
* validation_metric


# Aggregation
Скрипт агрегации получает на вход
* состояние агрегатора
* prediction_table.csv


Аггрегатор имеет
* Метод load_state(state)
      * state всегда ссылается на predictor_id. В случае взвешенной аггрегации state – это словарь predictor_id__to__weight
* Метод get_predictor_ids()
* Метод aggregate(predictions_dict)
      * Проверяет, что все predictor_id из его состояния есть в predictions_dict.keys()
      * Агрегирует :)
      * В случае взвешенного агрегатора:
            * scaled_preds = []
            * for predictor_id, weight in predictor_id__to__weight.items():
                  * scaled_preds.append(scale_preds(predictions_dict['predictor_id'], weight))
            * final_preds = merge_sorted_preds(scaled_preds)


predictions_dict будет поучаться с помощью функции get_predictions_dict(predictor_ids, dataset_split), которая по списку id и типу датасета (train/val) находит их в табличке и возвращает словарь {id → предсказания}



# Представление предсказаний
Сейчас я вижу три варианта представдения предсказаний: 
1. список предсказаний длиной с исходный датасет. Для строк с расскаладками,
      с которыми модель не умеет работать она предсказывает пустой список.
2. список предсказаний длиной с число элементов данной раскладки.
3. словарь длиной с число элементов данной раскладки 
      `индекс в исходном датасете` -> список предсказаний для данной кривой.
      Сюда же отнесу вариант, где предсказания представлены списком, каждый
      элемент которого - кортеж (номер_строки_в_исходном_датасете, список_предсказаний).
Также может быть добавлена метаинформация:
* название архтиектуры
* путь к весам
* название раскладки
* список индексов

Вариант, когда каждая модель выдает все 10_000 строк кажется
не совсем удобным: Когда мы подбираем наилучшие гиперпараметры
для аггрегации, удобно оценивать гиперпараметры по результатам
метрик именно на данной раскладке.


# Other
* Авторегрессионное использование трансформера сопровождается огромным числом повторных вычислений. Когда мы предсказываали предыдущий токен мы уже посчитали все quiries, keys и values на всех слоях для всех токенов, кроме последнего. Добавление кэширования может вдвое ускорить скорость предсказания 


# TODO
* Обновить predict; перенести туда word_generation_v2; переименовать скрипт; перенести часть размышлений в md
* Проверить, что скорость эпохи не уменьшилась, относительно prepare_batch_truncation
* Проверить, что предсказание и аггрегация работают
* Обновить обучение
* добавить collate_fn в dataset.py
* Написать новый trainer
* Изучить torchl_ightning trainer
* Сделать batch_first ветку, обучить там транфсормер, у которого dim encoder целиком равен dim decoder