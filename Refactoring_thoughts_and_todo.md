# General

Должен ли датасет получать на вход токенизатор (используется только в get_item).
Может быть, лучше чтобы он возвращал строку?


Обученная модель = 
* Имя раскладки
* Путь до весов
* Имя архитектуры

Нужно ли делить json на отдельные grid'ы?

Если я уж разделил все датасеты на отдельные grid'ы можно валидационной и тестовой выборке добавить отдельный файл original indexes, чтобы удобно производить валидацию и удобно делать submission.

Думаю, создание словаря [координаты → лейбл ближайшей буквы] нужно вынести в отдельный скрипт и сделать единожды.
Возможно, лучше это будет словарь [координаты → список расстояний до каждой из клавиш ]. Мб это поможет сделать линейную интерполяцию.

# Prediction

При предсказании предлагается заполнять таблицу prediction_table.csv:
* word_generator_id aka predictor_id
      * (определяется всем, кроме test_preds_path, val_preds_path, validation_metric)
* –
* Generator_type
* Generator_call_kwargs_json
* Model_architecture_name
* Model_weights_path
* –
* Grid_name
* –
* test_preds_path
* val_preds_path
* validation_metric


Кажется хорошей идеей сделтаь клаас Predictor, который будет иметь методы
* predict(dataset)
* save_predictions()
* __init__(word_generator)


Возникает вопрос, не должен ли это быть тот же класс, что и WordGenerator?
WordGenerator единственная разница - word_generator предсказывает для
одной кривой, а Predictor предсказывает для всего датасета.


# Aggregation
Скрипт агрегации получает на вход
* состояние агрегатора
* prediction_table.csv


Аггрегатор имеет
* Метод load_state(state)
      * state всегда ссылается на predictor_id. В случае взвешенной аггрегации state – это словарь predictor_id__to__weight
* Метод get_predictor_ids()
* Метод aggregate(predictions_dict)
      * Проверяет, что все predictor_id из его состояния есть в predictions_dict.keys()
      * Агрегирует :)
      * В случае взвешенного агрегатора:
            * scaled_preds = []
            * for predictor_id, weight in predictor_id__to__weight.items():
                  * scaled_preds.append(scale_preds(predictions_dict['predictor_id'], weight))
            * final_preds = merge_sorted_preds(scaled_preds)


predictions_dict будет поучаться с помощью функции get_predictions_dict(predictor_ids, dataset_split), которая по списку id и типу датасета (train/val) находит их в табличке и возвращает словарь {id → предсказания}



# Представление предсказаний
Сейчас я вижу три варианта представдения предсказаний: 
1. список предсказаний длиной с исходный датасет. Для строк с расскаладками,
      с которыми модель не умеет работать она предсказывает пустой список.
2. список предсказаний длиной с число элементов данной раскладки.
3. словарь длиной с число элементов данной раскладки 
      `индекс в исходном датасете` -> список предсказаний для данной кривой.
      Сюда же отнесу вариант, где предсказания представлены списком, каждый
      элемент которого - кортеж (номер_строки_в_исходном_датасете, список_предсказаний).
Также может быть добавлена метаинформация:
* название архтиектуры
* путь к весам
* название раскладки
* список индексов

Вариант, когда каждая модель выдает все 10_000 строк кажется
не совсем удобным: Когда мы подбираем наилучшие гиперпараметры
для аггрегации, удобно оценивать гиперпараметры по результатам
метрик именно на данной раскладке.


# TODO
* Обновить predict; перенести туда word_generation_v2; переименовать скрипт; перенести часть размышлений в md
* Проверить, что скорость эпохи не уменьшилась, относительно prepare_batch_truncation
* Проверить, что предсказание и аггрегация работают
* Обновить обучение
* добавить collate_fn в dataset.py
* Написать новый trainer
* Изучить torchl_ightning trainer
* Сделать batch_first ветку, обучить там транфсормер, у которого dim encoder целиком равен dim decoder