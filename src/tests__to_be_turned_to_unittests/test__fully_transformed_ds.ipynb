{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START OF TEST THAT PICKLED FULLY TRANSFORMED IS SAME AS SEPARATE INIT AND GETITEM TRANSFORM DATASET VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple, Optional, Set\n",
    "import pickle\n",
    "import json \n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from predict import get_grid_name_to_grid\n",
    "from nearest_key_lookup import ExtendedNearestKeyLookup\n",
    "from tokenizers import CharLevelTokenizerv2, KeyboardTokenizerv1, ALL_CYRILLIC_LETTERS_ALPHABET_ORD\n",
    "from transforms import InitTransform, GetItemTransform\n",
    "from dataset import CurveDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/fully_transformed_datasets/valid_ft.pkl', 'rb') as f:\n",
    "    valid_ds_pickled = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/fully_transformed_datasets/valid_ft_4_workers.pkl', 'rb') as f:\n",
    "    valid_ds_4_workers_pickled = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulating out-of-bounds coordinates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10000/6000000 [00:00<07:05, 14068.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ExtendedNearestKeyLookups...\n",
      "Creating datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:02, 4811.73it/s]                         \n"
     ]
    }
   ],
   "source": [
    "VAL_DS_PATH = '../data/data_separated_grid/valid__in_train_format.jsonl'\n",
    "\n",
    "\n",
    "def get_gridname_to_out_of_bounds_coords_dict(\n",
    "        data_paths: List[str], gridname_to_wh: dict,\n",
    "        total: Optional[int] = None\n",
    "        ) -> Dict[str, Set[Tuple[int, int]]]:\n",
    "    \"\"\"\n",
    "    Returns a dictionary with grid names as keys and lists of out of bounds coordinates as values.\n",
    "    \"\"\"\n",
    "    gname_to_out_of_bounds = {gname: set() for gname in gridname_to_wh.keys()}\n",
    "\n",
    "    for data_path in data_paths:\n",
    "        with open(data_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "            for line in tqdm(json_file, total=total):\n",
    "                json_data = json.loads(line)\n",
    "                curve = json_data['curve']\n",
    "                grid_name = curve['grid_name']\n",
    "                w, h = gridname_to_wh[grid_name]\n",
    "                X, Y = curve['x'], curve['y']\n",
    "                out_of_bounds = set((x, y) for x, y in zip(X, Y) \n",
    "                                    if x < 0 or x >= w or y < 0 or y >= h)\n",
    "                gname_to_out_of_bounds[grid_name].update(out_of_bounds)\n",
    "    return gname_to_out_of_bounds\n",
    "\n",
    "\n",
    "kb_tokenizer = KeyboardTokenizerv1()\n",
    "word_char_tokenizer = CharLevelTokenizerv2('../data/data_separated_grid/voc.txt')\n",
    "\n",
    "\n",
    "gridname_to_grid  = get_grid_name_to_grid('../data/data_separated_grid/gridname_to_grid.json')\n",
    "\n",
    "gname_to_wh = {\n",
    "    gname: (grid['width'], grid['height']) \n",
    "    for gname, grid in gridname_to_grid.items()\n",
    "}\n",
    "\n",
    "print(\"Accumulating out-of-bounds coordinates...\")\n",
    "gname_to_out_of_bounds = get_gridname_to_out_of_bounds_coords_dict(\n",
    "    [VAL_DS_PATH], gname_to_wh, total=6_000_000\n",
    ")\n",
    "\n",
    "print(\"Creating ExtendedNearestKeyLookups...\")\n",
    "nearest_key_candidates = ALL_CYRILLIC_LETTERS_ALPHABET_ORD\n",
    "gridname_to_nkl = {\n",
    "    gname: ExtendedNearestKeyLookup(grid, nearest_key_candidates, gname_to_out_of_bounds[gname])\n",
    "    for gname, grid in gridname_to_grid.items()\n",
    "}\n",
    "\n",
    "\n",
    "init_transform = InitTransform(\n",
    "    grid_name_to_nk_lookup=gridname_to_nkl,\n",
    "    kb_tokenizer=kb_tokenizer,\n",
    ")\n",
    "\n",
    "get_item_transform = GetItemTransform(\n",
    "    grid_name_to_wh=gname_to_wh,\n",
    "    word_tokenizer=word_char_tokenizer,\n",
    "    include_time=False,\n",
    "    include_velocities=True,\n",
    "    include_accelerations=True,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Creating datasets...\")\n",
    "val_ds = CurveDataset(\n",
    "    data_path=VAL_DS_PATH,\n",
    "    store_gnames = False,\n",
    "    init_transform=init_transform,\n",
    "    get_item_transform=get_item_transform,\n",
    "    total = 9_416,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_datasets_equal(ds1: CurveDataset, ds2: CurveDataset) -> None:\n",
    "    assert len(ds1) == len(ds2)\n",
    "    for data_sample1, data_sample2 in tqdm(zip(ds1, ds2), total = len(ds1)):\n",
    "        model_inputs1, target1 = data_sample1\n",
    "        model_inputs2, target2 = data_sample2\n",
    "        for mi1_tensor, mi2_tensor in zip(model_inputs1, model_inputs2):\n",
    "            assert mi1_tensor.equal(mi2_tensor)\n",
    "        assert target1.equal(target2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:17<00:00, 560.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed! All samples are equal!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_datasets_equal(valid_ds_pickled, val_ds)\n",
    "print(\"Test passed! All samples are equal!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 38911.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed! All samples are equal!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_datasets_equal(valid_ds_pickled, valid_ds_4_workers_pickled)\n",
    "print(\"Test passed! All samples are equal!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python ./src/create_and_save_fully_transformed_ds.py --jsonl_path ./data/data_separated_grid/train__default_only_no_errors__2023_10_31__03_26_16.jsonl --output_path ./train_default_grid_no_errors__2023_10_31_ft.pkl --vocab_path ./data/data_separated_grid/voc.txt --gridname_to_grid_path ./data/data_separated_grid/gridname_to_grid.json --n_workers 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END OF TEST THAT PICKLED FULLY TRANSFORMED IS SAME AS SEPARATE INIT AND GETITEM TRANSFORM DATASET VERSION"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
