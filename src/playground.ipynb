{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "\n",
    "# from model import SwipeCurveEncoderTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwipeCurveEncoderTransformer_old(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer-based Curve encoder takes in a sequence of vectors and creates a representation\n",
    "    of a swipe gesture on a samrtphone keyboard.\n",
    "    Each vector contains information about finger trajectory at a time step.\n",
    "    It contains:\n",
    "    * x coordinate\n",
    "    * y coordinate\n",
    "    * Optionally: t\n",
    "    * Optionally: dx/dt\n",
    "    * Optionally: dy/dt\n",
    "    * Optionally: keyboard key that has x and y coordinates within its boundaries\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # self.pos_encoder = PositionalEncoding(input_size, dropout)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(input_size, num_heads, hidden_size, dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class SwipeCurveDecoderTransformerv1(nn.Module):\n",
    "    \"\"\"\n",
    "    Decodes a swipe gesture representation into a sequence of characters.\n",
    "\n",
    "    Uses decoder transformer with masked attention to prevent the model from cheating.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout, activation):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.num_encoder_layers = num_encoder_layers\n",
    "        self.num_decoder_layers = num_decoder_layers\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout = dropout\n",
    "        self.activation = activation\n",
    "\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout, activation)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(self.decoder_layer, num_decoder_layers)\n",
    "\n",
    "    def forward(self, x, memory):\n",
    "        x = self.transformer_decoder(x, memory)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SwipeCurveTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    SwipeCurveTransformer is a sequence-to-sequence model that encodes a sequence of vectors\n",
    "    representing a swipe gesture into a sequence of characters.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.encoder = SwipeCurveEncoderTransformerv1(input_size, hidden_size, num_layers, num_heads, dropout)\n",
    "        self.decoder = SwipeCurveDecoderTransformerv1(input_size, num_heads, num_layers, num_layers, hidden_size, dropout, 'relu')\n",
    "        self.out = nn.Linear(input_size, input_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(y, x)\n",
    "        x = self.out(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "embed_dim must be divisible by num_heads",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\proshian\\Documents\\yandex_cup_2023_ml_neuroswipe\\src\\playground.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/proshian/Documents/yandex_cup_2023_ml_neuroswipe/src/playground.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(\u001b[39m10\u001b[39m, \u001b[39m32\u001b[39m, input_size)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/proshian/Documents/yandex_cup_2023_ml_neuroswipe/src/playground.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(\u001b[39m20\u001b[39m, \u001b[39m32\u001b[39m, input_size)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/proshian/Documents/yandex_cup_2023_ml_neuroswipe/src/playground.ipynb#X20sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m model \u001b[39m=\u001b[39m SwipeCurveTransformer(input_size, hidden_size, num_layers, num_layers, \u001b[39m0.1\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/proshian/Documents/yandex_cup_2023_ml_neuroswipe/src/playground.ipynb#X20sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m output \u001b[39m=\u001b[39m model(x, y)\n",
      "\u001b[1;32mc:\\Users\\proshian\\Documents\\yandex_cup_2023_ml_neuroswipe\\src\\playground.ipynb Cell 3\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/proshian/Documents/yandex_cup_2023_ml_neuroswipe/src/playground.ipynb#X20sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads \u001b[39m=\u001b[39m num_heads\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/proshian/Documents/yandex_cup_2023_ml_neuroswipe/src/playground.ipynb#X20sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout \u001b[39m=\u001b[39m dropout\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/proshian/Documents/yandex_cup_2023_ml_neuroswipe/src/playground.ipynb#X20sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder \u001b[39m=\u001b[39m SwipeCurveEncoderTransformerv1(input_size, hidden_size, num_layers, num_heads, dropout)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/proshian/Documents/yandex_cup_2023_ml_neuroswipe/src/playground.ipynb#X20sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder \u001b[39m=\u001b[39m SwipeCurveDecoderTransformerv1(input_size, num_heads, num_layers, num_layers, hidden_size, dropout, \u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/proshian/Documents/yandex_cup_2023_ml_neuroswipe/src/playground.ipynb#X20sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(input_size, input_size)\n",
      "\u001b[1;32mc:\\Users\\proshian\\Documents\\yandex_cup_2023_ml_neuroswipe\\src\\playground.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/proshian/Documents/yandex_cup_2023_ml_neuroswipe/src/playground.ipynb#X20sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout \u001b[39m=\u001b[39m dropout\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/proshian/Documents/yandex_cup_2023_ml_neuroswipe/src/playground.ipynb#X20sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# self.pos_encoder = PositionalEncoding(input_size, dropout)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/proshian/Documents/yandex_cup_2023_ml_neuroswipe/src/playground.ipynb#X20sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder_layer \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mTransformerEncoderLayer(input_size, num_heads, hidden_size, dropout)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/proshian/Documents/yandex_cup_2023_ml_neuroswipe/src/playground.ipynb#X20sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer_encoder \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mTransformerEncoder(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder_layer, num_layers)\n",
      "File \u001b[1;32mc:\\Users\\proshian\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:445\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.__init__\u001b[1;34m(self, d_model, nhead, dim_feedforward, dropout, activation, layer_norm_eps, batch_first, norm_first, device, dtype)\u001b[0m\n\u001b[0;32m    443\u001b[0m factory_kwargs \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m: device, \u001b[39m'\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m'\u001b[39m: dtype}\n\u001b[0;32m    444\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m--> 445\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attn \u001b[39m=\u001b[39m MultiheadAttention(d_model, nhead, dropout\u001b[39m=\u001b[39mdropout, batch_first\u001b[39m=\u001b[39mbatch_first,\n\u001b[0;32m    446\u001b[0m                                     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs)\n\u001b[0;32m    447\u001b[0m \u001b[39m# Implementation of Feedforward model\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear1 \u001b[39m=\u001b[39m Linear(d_model, dim_feedforward, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\proshian\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\activation.py:969\u001b[0m, in \u001b[0;36mMultiheadAttention.__init__\u001b[1;34m(self, embed_dim, num_heads, dropout, bias, add_bias_kv, add_zero_attn, kdim, vdim, batch_first, device, dtype)\u001b[0m\n\u001b[0;32m    967\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39m=\u001b[39m batch_first\n\u001b[0;32m    968\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim \u001b[39m=\u001b[39m embed_dim \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m num_heads\n\u001b[1;32m--> 969\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim \u001b[39m*\u001b[39m num_heads \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39m\"\u001b[39m\u001b[39membed_dim must be divisible by num_heads\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    971\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_qkv_same_embed_dim:\n\u001b[0;32m    972\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_proj_weight \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39mempty((embed_dim, embed_dim), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs))\n",
      "\u001b[1;31mAssertionError\u001b[0m: embed_dim must be divisible by num_heads"
     ]
    }
   ],
   "source": [
    "# let's test SwipeCurveTransformer on a tensor\n",
    "\n",
    "input_size = 5\n",
    "hidden_size = 10\n",
    "num_layers = 2\n",
    "\n",
    "x = torch.rand(10, 32, input_size)\n",
    "y = torch.rand(20, 32, input_size)\n",
    "\n",
    "model = SwipeCurveTransformer(input_size, hidden_size, num_layers, num_layers, 0.1)\n",
    "\n",
    "output = model(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SwipeCurveEncoderTransformerLSTM(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Transformer-based Curve encoder takes in a sequence of vectors and creates a representation\n",
    "#     of a swipe gesture on a samrtphone keyboard.\n",
    "#     Each vector contains information about finger trajectory at a time step.\n",
    "#     It contains:\n",
    "#     * x coordinate\n",
    "#     * y coordinate\n",
    "#     * Optionally: t\n",
    "#     * Optionally: dx/dt\n",
    "#     * Optionally: dy/dt\n",
    "#     * Optionally: keyboard key that has x and y coordinates within its boundaries\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, input_size, hidden_size, num_layers, num_heads, dropout):\n",
    "#         super().__init__()\n",
    "#         self.input_size = input_size\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.num_layers = num_layers\n",
    "#         self.num_heads = num_heads\n",
    "#         self.dropout = dropout\n",
    "\n",
    "#         # self.pos_encoder = PositionalEncoding(input_size, dropout)\n",
    "#         self.encoder_layer = nn.TransformerEncoderLayer(input_size, num_heads, hidden_size, dropout)\n",
    "#         self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers)\n",
    "#         self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         # x = self.pos_encoder(x)\n",
    "#         x = self.transformer_encoder(x)\n",
    "#         x, _ = self.lstm(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "\n",
    "class NeuroSwipeIterableDatasetv1(IterableDataset):\n",
    "    \"\"\"\n",
    "    Dataset class for NeuroSwipe dataset.\n",
    "    The dataset file weights over 3 GB and contains over 6 million swipe gestures.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_path):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_path (string): Path to the NeuroSwipe dataset in JSON format.\n",
    "                A custom version of the dataset is used:\n",
    "                \"grid\" property is replaced with \"grid_name\" property.\n",
    "        \"\"\"\n",
    "        self.json_file = open(data_path, \"r\", encoding=\"utf-8\")\n",
    "\n",
    "    def __del__(self):\n",
    "        self.json_file.close()\n",
    "\n",
    "    def _get_data_from_json_line(self, line):\n",
    "        \"\"\"\n",
    "        Parses a JSON line and returns a dictionary with data.\n",
    "        \"\"\"\n",
    "        data = json.loads(line)\n",
    "        word: str = data['word']\n",
    "\n",
    "        X_list = data['curve']['x']\n",
    "        Y_list = data['curve']['y']\n",
    "        T_list = data['curve']['t']\n",
    "\n",
    "        X = torch.tensor(X_list, dtype=torch.float32)\n",
    "        Y = torch.tensor(Y_list, dtype=torch.float32)\n",
    "        T = torch.tensor(T_list, dtype=torch.float32)\n",
    "\n",
    "        return X, Y, T, word\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for line in self.json_file:\n",
    "            yield self._get_data_from_json_line(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Optional, List, Tuple\n",
    "import array\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class NeuroSwipeDatasetv1(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for NeuroSwipe dataset.\n",
    "    The dataset file weights over 3 GB and contains over 6 million swipe gestures.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 data_path,\n",
    "                #  max_len: int,\n",
    "                #  word_tokenizer,\n",
    "                 add_velcities: bool = True,\n",
    "                 add_accelerations: bool = True,\n",
    "                 total: Optional[int] = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_path (string): Path to the NeuroSwipe dataset in JSON format.\n",
    "                A custom version of the dataset is used:\n",
    "                \"grid\" property is replaced with \"grid_name\" property.\n",
    "        \"\"\"\n",
    "        if add_accelerations and not add_velcities:\n",
    "\n",
    "            raise ValueError(\"Accelerations are supposed \\\n",
    "                             to be an addition to velocities. Add velocities.\")\n",
    "\n",
    "        self.add_velcities = add_velcities\n",
    "        self.add_accelerations = add_accelerations\n",
    "\n",
    "        self.data_list = []\n",
    "        self._set_data(data_path, self.data_list, total = total)\n",
    "    \n",
    "    def _set_data(self, data_path: str, data_list: list, total: Optional[int] = None):\n",
    "        with open(data_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "            for line in tqdm(json_file, total = total):\n",
    "                data_list.append(self._get_data_from_json_line(line))\n",
    "\n",
    "    def _get_dx_dt(self, X: torch.tensor, T: torch.tensor) -> List[float]:\n",
    "        \"\"\"\n",
    "        Calculates dx/dt for a list of x coordinates and a list of t coordinates.\n",
    "        \"\"\"\n",
    "        dx_dt = torch.zeros_like(X)\n",
    "        dx_dt[1:-1] = (X[2:] - X[:-2]) / (T[2:] - T[:-2])\n",
    "\n",
    "        # x0 x1 x2 x3\n",
    "        # t0 t1 t2 t3\n",
    "        # dx_dt[0] = 0\n",
    "        # dx_dt[1] = (x2 - x0) / (t2 - t0)\n",
    "        # dx_dt[2] = (x3 - x1) / (t3 - t1)\n",
    "        # dx_dt[3] = 0\n",
    "\n",
    "        return dx_dt\n",
    "\n",
    "    def _get_data_from_json_line(self, line) -> Tuple[list, list, list, str]:\n",
    "        \"\"\"\n",
    "        Parses a JSON line and returns a dictionary with data.\n",
    "        \"\"\"\n",
    "        data = json.loads(line)\n",
    "        word: str = data['word']\n",
    "        try:\n",
    "            X = array.array('h', data['curve']['x'])\n",
    "            Y = array.array('h', data['curve']['y'])\n",
    "            T = array.array('h', data['curve']['t'])\n",
    "        except:\n",
    "            print(data['curve']['x'])\n",
    "            print(data['curve']['y'])\n",
    "            print(data['curve']['t'])\n",
    "\n",
    "        return X, Y, T, word\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X_list, Y_list, T_list, word =  self.data_list[idx]\n",
    "\n",
    "        X = torch.tensor(X_list, dtype=torch.float32)\n",
    "        Y = torch.tensor(Y_list, dtype=torch.float32)\n",
    "        T = torch.tensor(T_list, dtype=torch.float32)\n",
    "\n",
    "        xyt = torch.cat(\n",
    "            [\n",
    "                X.reshape(-1, 1),\n",
    "                Y.reshape(-1, 1),\n",
    "                T.reshape(-1, 1)\n",
    "            ],\n",
    "            axis = 1\n",
    "        )\n",
    "\n",
    "        if self.add_velcities:\n",
    "            dx_dt = self._get_dx_dt(X, T)\n",
    "            dy_dt = self._get_dx_dt(Y, T)\n",
    "            xyt = torch.cat(\n",
    "                [\n",
    "                    xyt,\n",
    "                    dx_dt.reshape(-1, 1),\n",
    "                    dy_dt.reshape(-1, 1)\n",
    "                ],\n",
    "                axis = 1\n",
    "            )\n",
    "\n",
    "        if self.add_accelerations:\n",
    "            d2x_dt2 = self._get_dx_dt(dx_dt, T)\n",
    "            d2y_dt2 = self._get_dx_dt(dy_dt, T)\n",
    "            xyt = torch.cat(\n",
    "                [\n",
    "                    xyt,\n",
    "                    d2x_dt2.reshape(-1, 1),\n",
    "                    d2y_dt2.reshape(-1, 1)\n",
    "                ],\n",
    "                axis = 1\n",
    "            )\n",
    "    \n",
    "        return xyt, word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_path = \"../data/data_separated_grid/train.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1216087/6000000 [01:17<05:04, 15717.60it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\proshian\\Documents\\yandex_cup_2023_ml_neuroswipe\\src\\playground.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/proshian/Documents/yandex_cup_2023_ml_neuroswipe/src/playground.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m dataset \u001b[39m=\u001b[39m NeuroSwipeDatasetv1(data_path\u001b[39m=\u001b[39;49mtrain_dataset_path, total \u001b[39m=\u001b[39;49m \u001b[39m6_000_000\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\proshian\\Documents\\yandex_cup_2023_ml_neuroswipe\\src\\playground.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/proshian/Documents/yandex_cup_2023_ml_neuroswipe/src/playground.ipynb#W6sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/proshian/Documents/yandex_cup_2023_ml_neuroswipe/src/playground.ipynb#W6sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/proshian/Documents/yandex_cup_2023_ml_neuroswipe/src/playground.ipynb#W6sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m    data_path (string): Path to the NeuroSwipe dataset in JSON format.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/proshian/Documents/yandex_cup_2023_ml_neuroswipe/src/playground.ipynb#W6sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m        A custom version of the dataset is used:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/proshian/Documents/yandex_cup_2023_ml_neuroswipe/src/playground.ipynb#W6sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m        \"grid\" property is replaced with \"grid_name\" property.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/proshian/Documents/yandex_cup_2023_ml_neuroswipe/src/playground.ipynb#W6sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/proshian/Documents/yandex_cup_2023_ml_neuroswipe/src/playground.ipynb#W6sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_list \u001b[39m=\u001b[39m []\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/proshian/Documents/yandex_cup_2023_ml_neuroswipe/src/playground.ipynb#W6sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_data(data_path, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_list, total \u001b[39m=\u001b[39;49m total)\n",
      "\u001b[1;32mc:\\Users\\proshian\\Documents\\yandex_cup_2023_ml_neuroswipe\\src\\playground.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/proshian/Documents/yandex_cup_2023_ml_neuroswipe/src/playground.ipynb#W6sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_set_data\u001b[39m(\u001b[39mself\u001b[39m, data_path: \u001b[39mstr\u001b[39m, data_list: \u001b[39mlist\u001b[39m, total: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/proshian/Documents/yandex_cup_2023_ml_neuroswipe/src/playground.ipynb#W6sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(data_path, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m json_file:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/proshian/Documents/yandex_cup_2023_ml_neuroswipe/src/playground.ipynb#W6sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m tqdm(json_file, total \u001b[39m=\u001b[39m total):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/proshian/Documents/yandex_cup_2023_ml_neuroswipe/src/playground.ipynb#W6sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m             data_list\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_data_from_json_line(line))\n",
      "File \u001b[1;32mc:\\Users\\proshian\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\proshian\\AppData\\Local\\Programs\\Python\\Python310\\lib\\codecs.py:319\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_buffer_decode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, errors, final):\n\u001b[0;32m    315\u001b[0m     \u001b[39m# Overwrite this method in subclasses: It must decode input\u001b[39;00m\n\u001b[0;32m    316\u001b[0m     \u001b[39m# and return an (output, length consumed) tuple\u001b[39;00m\n\u001b[0;32m    317\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m\n\u001b[1;32m--> 319\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    320\u001b[0m     \u001b[39m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[0;32m    321\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer \u001b[39m+\u001b[39m \u001b[39minput\u001b[39m\n\u001b[0;32m    322\u001b[0m     (result, consumed) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer_decode(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merrors, final)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = NeuroSwipeDatasetv1(data_path=train_dataset_path, total = 6_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 32, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 10\n",
    "seq_len = 32\n",
    "num_features = 3\n",
    "\n",
    "encoder = SwipeCurveEncoderTransformerv1(input_size=3, hidden_size=128, num_layers=1, num_heads=1, dropout=0.1)\n",
    "encoder(torch.rand(batch_size, seq_len, num_features)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "seq_len = 32\n",
    "num_features = 3\n",
    "\n",
    "encoder = SwipeCurveEncoderTransformerLSTM(\n",
    "    input_size=3, hidden_size=128, num_layers=1, num_heads=1, dropout=0.1)\n",
    "\n",
    "encoder_out = encoder(torch.rand(batch_size, seq_len, num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 32, 256])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_out[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\proshian\\Documents\\yandex_cup_2023_ml_neuroswipe\\src\\playground.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/proshian/Documents/yandex_cup_2023_ml_neuroswipe/src/playground.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m encoder_out[\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mshape\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "encoder_out[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# организовать padding encoder'a и decoder'a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SwipeCurveTransformerEncoderv1(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer-based Curve encoder takes in a sequence of vectors and creates a representation\n",
    "    of a swipe gesture on a samrtphone keyboard.\n",
    "    Each vector contains information about finger trajectory at a time step.\n",
    "    It contains:\n",
    "    * x coordinate\n",
    "    * y coordinate\n",
    "    * Optionally: t\n",
    "    * Optionally: dx/dt\n",
    "    * Optionally: dy/dt\n",
    "    * Optionally: keyboard key that has x and y coordinates within its boundaries\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, d_model,\n",
    "                 dim_feedforward, num_layers, num_heads,\n",
    "                 padding_idx = 0,\n",
    "                 dropout = 0.1):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        ----------\n",
    "        input_size: int\n",
    "            Size of input vectors.\n",
    "        d_model: int\n",
    "            Size of the embeddings (output vectors).\n",
    "            Should be equal to char embedding size of the decoder.\n",
    "        dim_feedforward: int\n",
    "        num_layers: int\n",
    "            Number of encoder layers including the first layer.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # self.input_size = input_size\n",
    "        # self.d_model = d_model\n",
    "        # self.dim_feedforward  = dim_feedforward\n",
    "        # self.num_layers = num_layers\n",
    "        # self.num_heads = num_heads\n",
    "        # self.dropout = dropout\n",
    "\n",
    "        # self.pos_encoder = PositionalEncoding(input_size, dropout)\n",
    "        self.first_encoder_layer = nn.TransformerEncoderLayer(input_size, num_heads, dim_feedforward, dropout)\n",
    "        self.liner = nn.Linear(input_size, d_model)  # to convert embedding to d_model size\n",
    "        num_layer_after_first = num_layers - 1\n",
    "        if num_layer_after_first > 0:\n",
    "            encoder_layer = nn.TransformerEncoderLayer(input_size, num_heads, dim_feedforward, dropout)\n",
    "            self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        else:\n",
    "            self.transformer_encoder = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x = self.pos_encoder(x)\n",
    "        x = self.first_encoder_layer(x)\n",
    "        x = self.liner(x)\n",
    "        if self.transformer_encoder:\n",
    "            x = self.transformer_encoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SwipeCurveTransformerDecoderv1(nn.Module):\n",
    "    \"\"\"\n",
    "    Decodes a swipe gesture representation into a sequence of characters.\n",
    "\n",
    "    Uses decoder transformer with masked attention to prevent the model from cheating.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, char_emb_size, nhead, num_decoder_layers, dim_feedforward, dropout, activation = F.relu):\n",
    "        super().__init__()\n",
    "\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(char_emb_size, nhead, dim_feedforward, dropout, activation)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(self.decoder_layer, num_decoder_layers)\n",
    "        self.out = nn.Linear(char_emb_size, char_emb_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "    \n",
    "    def forward(self, x, memory, tgt_mask):\n",
    "        x = self.transformer_decoder(x, memory, tgt_mask=tgt_mask)\n",
    "        x = self.out(x)\n",
    "        # x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    " \n",
    "    \n",
    "\n",
    "\n",
    "class SwipeCurveTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    SwipeCurveTransformer is a sequence-to-sequence model that encodes a sequence of vectors\n",
    "    representing a swipe gesture into a sequence of characters.\n",
    "    \"\"\"\n",
    "\n",
    "    def _get_mask(self, max_seq_len: int):\n",
    "        \"\"\"\n",
    "        Returns a mask for the decoder transformer.\n",
    "        \"\"\"\n",
    "        mask = torch.triu(torch.ones(max_seq_len, max_seq_len), diagonal=1)\n",
    "        mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "        return mask\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 curv_emb_size,\n",
    "                 char_emb_size,\n",
    "                 char_vocab_size,\n",
    "                 num_encoder_layers,\n",
    "                 num_decoder_layers,\n",
    "                 dim_feedforward,\n",
    "                 num_heads,\n",
    "                 dropout,\n",
    "                 activation = F.relu,\n",
    "                 max_out_seq_len):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.char_embedding = nn.Embedding(char_vocab_size, char_emb_size)\n",
    "\n",
    "        self.encoder = SwipeCurveTransformerEncoderv1(\n",
    "            input_size, curv_emb_size, dim_feedforward, num_encoder_layers, num_heads, dropout)\n",
    "        self.decoder = SwipeCurveTransformerDecoderv1(\n",
    "            char_emb_size, num_heads, num_decoder_layers, dim_feedforward, dropout, activation, max_out_seq_len)\n",
    "        self.out = nn.Linear(char_emb_size, char_emb_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "\n",
    "        self.mask = self._get_mask(max_out_seq_len)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = self.encoder(x)\n",
    "        y = self.char_embedding(y)\n",
    "        x = self.decoder(y, x, tgt_mask=self.mask)\n",
    "        x = self.out(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 32, 128])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = SwipeCurveTransformerEncoderv1(input_size=3, d_model=128, dim_feedforward=128, num_layers=1, num_heads=1, dropout=0.1)\n",
    "\n",
    "batch_size = 10\n",
    "seq_len = 32\n",
    "num_features = 3\n",
    "\n",
    "encoder(torch.rand(batch_size, seq_len, num_features)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = SwipeCurveTransformerDecoderv1(char_emb_size=128, nhead=1, num_decoder_layers=1, dim_feedforward=128, dropout=0.1)\n",
    "\n",
    "batch_size = 10\n",
    "seq_len = 32\n",
    "char_emb_size = 128\n",
    "\n",
    "def get_mask(max_seq_len: int):\n",
    "    \"\"\"\n",
    "    Returns a mask for the decoder transformer.\n",
    "    \"\"\"\n",
    "    mask = torch.triu(torch.ones(max_seq_len, max_seq_len), diagonal=1)\n",
    "    mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "    return mask\n",
    "\n",
    "target_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "\n",
    "decoder(torch.rand(batch_size, seq_len, char_emb_size), torch.rand(batch_size, seq_len, char_emb_size), tgt_mask=torch.rand(seq_len, seq_len).masked_fill(torch.rand(seq_len, seq_len) > 0.5, float('-inf'))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = SwipeCurveTransformerDecoderv1()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
