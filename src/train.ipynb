{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-03-05T13:32:00.094133Z","iopub.status.busy":"2024-03-05T13:32:00.093760Z","iopub.status.idle":"2024-03-05T13:32:00.127929Z","shell.execute_reply":"2024-03-05T13:32:00.126891Z","shell.execute_reply.started":"2024-03-05T13:32:00.094102Z"},"trusted":true},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# !git clone https://github.com/proshian/yandex-cup-2023-ml-neuroswipe.git\n","# %cd yandex-cup-2023-ml-neuroswipe\n","# ! git checkout datasetv4"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ! pip install gdown\n","# ! python ./src/downloaders/download_weights.py"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# !pip install dvc --quiet\n","# !pip install dvc_gdrive --quiet"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-05T13:32:13.638027Z","iopub.status.busy":"2024-03-05T13:32:13.637167Z","iopub.status.idle":"2024-03-05T13:32:16.541166Z","shell.execute_reply":"2024-03-05T13:32:16.540264Z","shell.execute_reply.started":"2024-03-05T13:32:13.637992Z"},"trusted":true},"outputs":[],"source":["# %cd /kaggle/working/yandex-cup-2023-ml-neuroswipe\n","# ! git pull\n","# ! git checkout datasetv4"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-05T13:34:04.700384Z","iopub.status.busy":"2024-03-05T13:34:04.699321Z","iopub.status.idle":"2024-03-05T13:34:04.722431Z","shell.execute_reply":"2024-03-05T13:34:04.721436Z","shell.execute_reply.started":"2024-03-05T13:34:04.700346Z"},"trusted":true},"outputs":[],"source":["%cd /kaggle/working/yandex-cup-2023-ml-neuroswipe/src"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-03-05T13:34:06.642895Z","iopub.status.busy":"2024-03-05T13:34:06.642094Z","iopub.status.idle":"2024-03-05T13:34:06.660046Z","shell.execute_reply":"2024-03-05T13:34:06.658864Z","shell.execute_reply.started":"2024-03-05T13:34:06.642864Z"},"trusted":true},"outputs":[],"source":["############# Script arguments emulation #############\n","\n","GRID_NAME = \"default\"\n","BATCH_SIZE = 320\n","IN_KAGGLE = False\n","RANDOM_SEED = 12\n","\n","DATA_ROOT = \"../data/data_separated_grid\"\n","MODELS_DIR = \"../data/trained_models/m1\""]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-03-05T13:34:07.001747Z","iopub.status.busy":"2024-03-05T13:34:07.000826Z","iopub.status.idle":"2024-03-05T13:34:22.178286Z","shell.execute_reply":"2024-03-05T13:34:22.177247Z","shell.execute_reply.started":"2024-03-05T13:34:07.001712Z"},"trusted":true},"outputs":[],"source":["import os\n","import json\n","import typing as tp\n","import traceback\n","from datetime import datetime\n","import copy\n","\n","import torch\n","# import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from tqdm.notebook import tqdm\n","import numpy as np\n","from torch.utils.tensorboard import SummaryWriter\n","\n","\n","from model import SwipeCurveTransformer, get_m1_bigger_model\n","from tokenizers import CharLevelTokenizerv2, KeyboardTokenizerv1\n","from tokenizers import ALL_CYRILLIC_LETTERS_ALPHABET_ORD\n","from dataset import CurveDataset, CollateFn\n","from word_generators import GreedyGenerator\n","from nearest_key_lookup import ExtendedNearestKeyLookup\n","from transforms import InitTransform, GetItemTransform"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-05T13:34:22.180483Z","iopub.status.busy":"2024-03-05T13:34:22.179917Z","iopub.status.idle":"2024-03-05T13:34:22.251490Z","shell.execute_reply":"2024-03-05T13:34:22.250413Z","shell.execute_reply.started":"2024-03-05T13:34:22.180455Z"},"trusted":true},"outputs":[],"source":["################ Other constants ####################\n","GRID_NAME_TO_DS_PATHS = {\n","    \"extra\": {\n","        \"train\": os.path.join(DATA_ROOT, \"train__extra_only_no_errors__2023_11_01__19_49_14.jsonl\"),\n","        \"val\": os.path.join(DATA_ROOT, \"valid__in_train_format__extra_only.jsonl\")\n","    },\n","    \"default\": {\n","        \"train\": os.path.join(DATA_ROOT, \"train__default_only_no_errors__2023_10_31__03_26_16.jsonl\"),\n","        \"val\": os.path.join(DATA_ROOT, \"valid__in_train_format__default_only.jsonl\")\n","    }\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-03-05T13:34:22.253253Z","iopub.status.busy":"2024-03-05T13:34:22.252900Z","iopub.status.idle":"2024-03-05T13:34:22.327544Z","shell.execute_reply":"2024-03-05T13:34:22.326596Z","shell.execute_reply.started":"2024-03-05T13:34:22.253221Z"},"trusted":true},"outputs":[],"source":["# if IN_KAGGLE:\n","#     DATA_ROOT = \"/kaggle/input/neuroswipe-defualt-only-v1\"\n","#     MODELS_DIR = \"\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-03-05T13:34:22.330854Z","iopub.status.busy":"2024-03-05T13:34:22.330376Z","iopub.status.idle":"2024-03-05T13:34:22.399166Z","shell.execute_reply":"2024-03-05T13:34:22.398335Z","shell.execute_reply.started":"2024-03-05T13:34:22.330815Z"},"trusted":true},"outputs":[],"source":["def init_random_seed(value):\n","    # random.seed(value)\n","    np.random.seed(value)\n","    torch.manual_seed(value)\n","    torch.cuda.manual_seed(value)\n","    # torch.backends.cudnn.deterministic = True"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-03-05T13:34:22.400592Z","iopub.status.busy":"2024-03-05T13:34:22.400292Z","iopub.status.idle":"2024-03-05T13:34:22.469443Z","shell.execute_reply":"2024-03-05T13:34:22.468513Z","shell.execute_reply.started":"2024-03-05T13:34:22.400568Z"},"trusted":true},"outputs":[],"source":["def get_grid(grid_name: str, grids_path: str) -> dict:\n","    with open(grids_path, \"r\", encoding=\"utf-8\") as f:\n","        return json.load(f)[grid_name]"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-03-05T13:34:22.471017Z","iopub.status.busy":"2024-03-05T13:34:22.470725Z","iopub.status.idle":"2024-03-05T13:34:22.545009Z","shell.execute_reply":"2024-03-05T13:34:22.544015Z","shell.execute_reply.started":"2024-03-05T13:34:22.470994Z"},"trusted":true},"outputs":[],"source":["from typing import List, Dict, Tuple, Optional, Set\n","\n","def get_gridname_to_out_of_bounds_coords_dict(\n","        data_paths: List[str], gridname_to_wh: dict,\n","        total: Optional[int] = None\n","        ) -> Dict[str, Set[Tuple[int, int]]]:\n","    \"\"\"\n","    Returns a dictionary with grid names as keys and lists of out of bounds coordinates as values.\n","    \"\"\"\n","    gname_to_out_of_bounds = {gname: set() for gname in gridname_to_wh.keys()}\n","\n","    for data_path in data_paths:\n","        with open(data_path, \"r\", encoding=\"utf-8\") as json_file:\n","            for line in tqdm(json_file, total=total):\n","                json_data = json.loads(line)\n","                curve = json_data['curve']\n","                grid_name = curve['grid_name']\n","                w, h = gridname_to_wh[grid_name]\n","                X, Y = curve['x'], curve['y']\n","                out_of_bounds = set((x, y) for x, y in zip(X, Y) \n","                                    if x < 0 or x >= w or y < 0 or y >= h)\n","                gname_to_out_of_bounds[grid_name].update(out_of_bounds)\n","    return gname_to_out_of_bounds"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-03-05T13:34:22.546847Z","iopub.status.busy":"2024-03-05T13:34:22.546594Z","iopub.status.idle":"2024-03-05T13:34:22.620707Z","shell.execute_reply":"2024-03-05T13:34:22.619626Z","shell.execute_reply.started":"2024-03-05T13:34:22.546826Z"},"trusted":true},"outputs":[],"source":["def get_datasets(grid_name: str, grid_name_to_grid_path: str,\n","                 train_data_path: str, val_data_path: str,\n","                 nearest_key_candidates: tp.Set[str],\n","                 kb_tokenizer: KeyboardTokenizerv1,\n","                 word_char_tokenizer: CharLevelTokenizerv2\n","                 ) -> tuple[CurveDataset, CurveDataset]:\n","    \n","    gridname_to_grid  = {grid_name: get_grid(grid_name, grid_name_to_grid_path)}\n","\n","    gname_to_wh = {\n","        gname: (grid['width'], grid['height']) \n","        for gname, grid in gridname_to_grid.items()\n","    }\n","    \n","    print(\"Accumulating out-of-bounds coordinates...\")\n","    gname_to_out_of_bounds = get_gridname_to_out_of_bounds_coords_dict(\n","        [train_data_path, val_data_path], gname_to_wh, total=6_000_000\n","    )\n","    \n","    print(\"Creating ExtendedNearestKeyLookups...\")\n","    gridname_to_nkl = {\n","        gname: ExtendedNearestKeyLookup(grid, nearest_key_candidates, gname_to_out_of_bounds[gname])\n","        for gname, grid in gridname_to_grid.items()\n","    }\n","    \n","    \n","    init_transform = InitTransform(\n","        grid_name_to_nk_lookup=gridname_to_nkl,\n","        kb_tokenizer=kb_tokenizer,\n","    )\n","\n","    get_item_transform = GetItemTransform(\n","        grid_name_to_wh=gname_to_wh,\n","        word_tokenizer=word_char_tokenizer,\n","        include_time=False,\n","        include_velocities=True,\n","        include_accelerations=True,\n","    )\n","\n","    \n","\n","\n","    val_ds = CurveDataset(\n","        data_path=val_data_path,\n","        store_gnames = False,\n","        init_transform=init_transform,\n","        get_item_transform=get_item_transform,\n","        total = 9_416,\n","    )\n","    \n","    return val_ds"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-03-05T13:35:34.572854Z","iopub.status.busy":"2024-03-05T13:35:34.572049Z","iopub.status.idle":"2024-03-05T13:35:34.652887Z","shell.execute_reply":"2024-03-05T13:35:34.651618Z","shell.execute_reply.started":"2024-03-05T13:35:34.572818Z"},"trusted":true},"outputs":[],"source":["init_random_seed(RANDOM_SEED)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-03-05T13:35:34.908170Z","iopub.status.busy":"2024-03-05T13:35:34.907560Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Accumulating out-of-bounds coordinates...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7c7f297d00734fd19cfaaff6c4ae7e53","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/6000000 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"99ca64e5e8fa4500b4e98f60f3623570","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/6000000 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Creating ExtendedNearestKeyLookups...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 9416/9416 [00:01<00:00, 5503.85it/s]\n"]}],"source":["# Pickling the dataset would be great to not waste\n","# around 20 minutes creating train_dataset.\n","\n","kb_tokenizer = KeyboardTokenizerv1()\n","voc_path=os.path.join(DATA_ROOT, \"voc.txt\")\n","word_char_tokenizer = CharLevelTokenizerv2(voc_path)\n","\n","val_dataset = get_datasets(\n","    grid_name=GRID_NAME,\n","    grid_name_to_grid_path=os.path.join(DATA_ROOT, \"gridname_to_grid.json\"),\n","    train_data_path = GRID_NAME_TO_DS_PATHS[GRID_NAME]['train'],\n","    val_data_path = GRID_NAME_TO_DS_PATHS[GRID_NAME]['val'],\n","    nearest_key_candidates = ALL_CYRILLIC_LETTERS_ALPHABET_ORD,\n","    kb_tokenizer=kb_tokenizer,\n","    word_char_tokenizer=word_char_tokenizer,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["transformer = get_m1_bigger_model(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def cross_entropy_with_reshape(pred, target, ignore_index=-100, label_smoothing=0.0):\n","    \"\"\"\n","    pred - BatchSize x TargetLen x VocabSize\n","    target - BatchSize x TargetLen\n","    \"\"\"\n","    pred_flat = pred.view(-1, pred.shape[-1])  # BatchSize*TargetLen x VocabSize\n","    target_flat = target.reshape(-1)  # BatchSize*TargetLen\n","    return F.cross_entropy(pred_flat,\n","                           target_flat,\n","                           ignore_index=ignore_index,\n","                           label_smoothing=label_smoothing)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def lr_scheduler(optimizer):\n","    return torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n","                                                      patience=20,\n","                                                      factor=0.5,\n","                                                      verbose=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def move_all_to_device(x, device):\n","    if torch.is_tensor(x):\n","        return x.to(device)\n","    elif not isinstance(x, (list, tuple)):\n","        raise ValueError(f'Unexpected data type {type(x)}')\n","    new_x = []\n","    for el in x:\n","        if not torch.is_tensor(el):\n","            raise ValueError(f'Unexpected data type {type(el)}')\n","        new_x.append(el.to(device))\n","    return new_x"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["collate_fn = CollateFn(\n","    word_pad_idx = word_char_tokenizer.char_to_idx['<pad>'], batch_first = False)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Протестируем корректность collate_fn (вызывается неявно в DataLoader)\n","\n","batch_size = 6\n","\n","\n","PAD_CHAR_TOKEN = word_char_tokenizer.char_to_idx[\"<pad>\"]\n","\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False,\n","                              num_workers=0, collate_fn=collate_fn)\n","\n","\n","dataset_els = [train_dataset[i] for i in range(batch_size)]\n","unproc_batch_x, unproc_batch_y = zip(*dataset_els)\n","\n","batch_x, batch_y = next(iter(train_dataloader))\n","\n","\n","############### Проверка корректности batch_y ###################\n","max_out_seq_len = max([len(y) for y in unproc_batch_y])\n","\n","assert batch_y.shape == (max_out_seq_len, batch_size)\n","\n","\n","for i in range(batch_size):\n","    assert (batch_y[:len(unproc_batch_y[i]), i] == unproc_batch_y[i]).all()\n","    assert (batch_y[len(unproc_batch_y[i]):, i] == PAD_CHAR_TOKEN).all()\n","\n","print(\"batch_y is correct\")\n","\n","\n","\n","############### Проверка корректности batch_x ###################\n","unproc_batch_traj_feats, unproc_batch_kb_tokens, unproc_batch_dec_in_char_seq = zip(*unproc_batch_x)\n","\n","(traj_feats, kb_tokens, dec_in_char_seq, traj_pad_mask, word_pad_mask) = batch_x\n","\n","\n","# каждая сущность, полученная выше из unpoc_batch_x - это tuple длины batch_size.\n","# Например, unproc_batch_traj_feats[i] = train_dataset[i][0][0]\n","\n","N_TRAJ_FEATS = 6\n","max_curve_len = max([el.shape[0] for el in unproc_batch_traj_feats]) \n","\n","assert max_curve_len == max([el.shape[0] for el in unproc_batch_kb_tokens])\n","\n","assert traj_feats.shape == (max_curve_len, batch_size, N_TRAJ_FEATS)\n","assert kb_tokens.shape == (max_curve_len, batch_size)\n","assert dec_in_char_seq.shape == (max_out_seq_len, batch_size)\n","assert traj_pad_mask.shape == (batch_size, max_curve_len)\n","assert word_pad_mask.shape == (batch_size, max_out_seq_len)\n","\n","\n","for i in range(batch_size):\n","    assert (traj_feats[:len(unproc_batch_traj_feats[i]), i] == unproc_batch_traj_feats[i]).all()\n","    assert (kb_tokens[:len(unproc_batch_kb_tokens[i]), i] == unproc_batch_kb_tokens[i]).all()\n","\n","    assert (dec_in_char_seq[:len(unproc_batch_dec_in_char_seq[i]), i] == unproc_batch_dec_in_char_seq[i]).all()\n","    assert (dec_in_char_seq[len(unproc_batch_dec_in_char_seq[i]):, i] == PAD_CHAR_TOKEN).all()\n","\n","    assert (traj_pad_mask[i, :len(unproc_batch_traj_feats[i])] == False).all()\n","    assert (traj_pad_mask[i, len(unproc_batch_traj_feats[i]):] == True).all()\n","    \n","    assert (word_pad_mask[i, :len(unproc_batch_dec_in_char_seq[i])] == False).all()\n","    assert (word_pad_mask[i, len(unproc_batch_dec_in_char_seq[i]):] == True).all()\n","\n","print(\"batch_x is correct\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from typing import List\n","\n","\n","def predict_greedy_raw(dataset,\n","                       greedy_word_generator: GreedyGenerator,\n","                      ) -> List[List[str]]:\n","    \"\"\"\n","    Creates predictions using greedy generation.\n","\n","    Supposed to be used with a dataset of a single grid\n","    \n","    Arguments:\n","    ----------\n","    dataset: NeuroSwipeDatasetv2\n","    grid_name_to_greedy_generator: dict\n","        Dict mapping grid names to GreedyGenerator objects.\n","    \"\"\"\n","    preds = [None] * len(dataset)\n","\n","    for data in tqdm(enumerate(dataset), total=len(dataset)):\n","        i, ((xyt, kb_tokens, _), _) = data\n","\n","        pred = greedy_word_generator.generate_word_only(xyt, kb_tokens)\n","        pred = pred.removeprefix(\"<sos>\")\n","        preds[i] = pred\n","\n","    return preds\n","\n","\n","def get_targets(dataset: CurveDataset) -> tp.List[str]:\n","    targets = []\n","    for _, target_tokens in dataset:\n","        # Last token is <eos>.\n","        target_str = word_char_tokenizer.decode(target_tokens[:-1])\n","        targets.append(target_str)\n","    return targets\n","\n","\n","def get_accuracy(preds, targets) -> float:\n","    return sum(pred == target for pred, target \n","               in zip(preds, targets)) / len(targets)\n","\n","\n","def get_greedy_generator_accuracy(val_dataset, model, \n","                                  word_char_tokenizer, device) -> float:\n","    val_targets = get_targets(val_dataset)\n","    greedy_generator = GreedyGenerator(model, word_char_tokenizer, device)\n","    greedy_preds = predict_greedy_raw(val_dataset, greedy_generator)\n","    return get_accuracy(greedy_preds, val_targets)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["###################### протестируем predict_greedy_raw ######################\n","\n","\n","MODEL_TO_TEST_GREEDY_GEN__PATH = \"../data/trained_models_for_final_submit/m1_bigger/\" \\\n","    \"m1_bigger_v2__2023_11_11__14_29_37__0.13679_default_l2_0_ls0_switch_0.pt\"\n","\n","# Leads to super slow inference.  I think it's due to \n","# high price of operations on small-amplitude floats.\n","# MODEL_TO_TEST_GREEDY_GEN__PATH = None\n","\n","\n","def test_greedy_generator(val_dataset, model_getter, model_weights, word_char_tokenizer, device) -> float:\n","    \n","    model = model_getter(device, model_weights)\n","\n","    return get_greedy_generator_accuracy(val_dataset, model, word_char_tokenizer, device)\n","\n","\n","\n","test_greedy_generator(val_dataset, get_m1_bigger_model, MODEL_TO_TEST_GREEDY_GEN__PATH, word_char_tokenizer, device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def train_eval_loop(model, train_dataset, val_dataset, criterion,\n","                    tb, epoch_start, lr=1e-4, epoch_n=10, batch_size=32,\n","                    collate_fn = None,\n","                    device=None, early_stopping_patience=20, l2_reg_alpha=0,\n","                    max_batches_per_epoch_train=10000,\n","                    max_batches_per_epoch_val=1000,\n","                    optimizer_ctor=None,\n","                    lr_scheduler_ctor=None,\n","                    shuffle_train=True,\n","                    label_smoothing = 0.0,\n","                    dataloader_workers_n=0,\n","                    criterion_ignore_index = -100,\n","                    model_name_postfix = \"\",\n","                    model_save_root = \".\",\n","                    ):\n","    \"\"\"\n","    Цикл для обучения модели. После каждой эпохи качество модели оценивается по отложенной выборке.\n","    :param model: torch.nn.Module - обучаемая модель\n","    :param train_dataset: torch.utils.data.Dataset - данные для обучения\n","    :param val_dataset: torch.utils.data.Dataset - данные для оценки качества\n","    :param criterion: функция потерь для настройки модели\n","    :param lr: скорость обучения\n","    :param epoch_n: максимальное количество эпох\n","    :param batch_size: количество примеров, обрабатываемых моделью за одну итерацию\n","    :param device: cuda/cpu - устройство, на котором выполнять вычисления\n","    :param early_stopping_patience: наибольшее количество эпох, в течение которых допускается\n","        отсутствие улучшения модели, чтобы обучение продолжалось.\n","    :param l2_reg_alpha: коэффициент L2-регуляризации\n","    :param max_batches_per_epoch_train: максимальное количество итераций на одну эпоху обучения\n","    :param max_batches_per_epoch_val: максимальное количество итераций на одну эпоху валидации\n","    :return: кортеж из двух элементов:\n","        - среднее значение функции потерь на валидации на лучшей эпохе\n","        - лучшая модель\n","    \"\"\"\n","    if device is None:\n","        device =  torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    model.to(device)\n","\n","    if optimizer_ctor is None:\n","        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2_reg_alpha)\n","    else:\n","        optimizer = optimizer_ctor(model.parameters(), lr=lr)\n","\n","    if lr_scheduler_ctor is not None:\n","        lr_scheduler = lr_scheduler_ctor(optimizer)\n","    else:\n","        lr_scheduler = None\n","\n","    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle_train,\n","                                        num_workers=dataloader_workers_n, collate_fn=collate_fn)\n","    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n","                                      num_workers=dataloader_workers_n, collate_fn=collate_fn)\n","\n","    best_val_loss = float('inf')\n","    best_epoch_i = 0\n","\n","    best_model_path = \"m1_bigger_v2.pt\"\n","    \n","    n_train_examples_in_epoch = (batch_size * max_batches_per_epoch_train \n","                                 if max_batches_per_epoch_train < len(train_dataset) // batch_size\n","                                 else len(train_dataset))\n","\n","    if os.path.exists(best_model_path):\n","        best_model.load_state_dict(torch.load(best_model_path))\n","        print(f\"Загружено состояние модели {best_model_path}\")\n","\n","    for epoch_i in tqdm(range(epoch_start, epoch_start + epoch_n), position = 0):\n","        try:\n","            model.train()\n","            mean_train_loss = 0\n","            train_batches_n = 0\n","            for batch_i, (batch_x, batch_y) in tqdm(enumerate(train_dataloader), total = min(max_batches_per_epoch_train, len(train_dataset) // batch_size), position=1, leave = False):\n","                if batch_i > max_batches_per_epoch_train:\n","                    break\n","                    \n","\n","                batch_x, batch_y = [move_all_to_device(el, device) for el in (batch_x, batch_y)]\n","\n","                pred = model(*batch_x)\n","                loss = criterion(pred, batch_y, ignore_index = criterion_ignore_index, \n","                                 label_smoothing=label_smoothing)\n","\n","                model.zero_grad()\n","                loss.backward()\n","\n","                optimizer.step()\n","\n","                mean_train_loss += float(loss)\n","                train_batches_n += 1\n","\n","            mean_train_loss /= train_batches_n\n","            \n","            print('Среднее значение функции потерь на обучении', mean_train_loss)\n","\n","            tb.add_scalar('mean_loss/train', mean_train_loss, epoch_i * n_train_examples_in_epoch)\n","\n","\n","\n","            model.eval()\n","            mean_val_loss = 0\n","            val_batches_n = 0\n","\n","            with torch.no_grad():\n","                for batch_i, (batch_x, batch_y) in enumerate(val_dataloader):\n","                    if batch_i > max_batches_per_epoch_val:\n","                        break\n","\n","                    batch_x, batch_y = [move_all_to_device(el, device) for el in (batch_x, batch_y)]\n","\n","                    pred = model(*batch_x)\n","                    loss = criterion(pred, batch_y, \n","                                     ignore_index = criterion_ignore_index, \n","                                     label_smoothing=label_smoothing)\n","\n","                    mean_val_loss += float(loss)\n","                    val_batches_n += 1\n","\n","            mean_val_loss /= val_batches_n\n","            print('Среднее значение функции потерь на валидации', mean_val_loss)\n","            tb.add_scalar('mean_loss/val', mean_val_loss, epoch_i * n_train_examples_in_epoch)\n","\n","            if mean_val_loss < best_val_loss:\n","                best_epoch_i = epoch_i\n","                best_val_loss = mean_val_loss\n","                best_model = copy.deepcopy(model)\n","                torch.save(model.state_dict(), os.path.join(model_save_root, best_model_path))\n","                \n","                cur_time = \"{:%Y_%m_%d__%H_%M_%S}\".format(datetime.now())\n","                \n","               \n","                greedy_accuracy = get_greedy_generator_accuracy(val_dataset, model, word_char_tokenizer, device)\n","                tb.add_scalar('greedy_accuracy/val', greedy_accuracy, epoch_i * n_train_examples_in_epoch)\n","                \n","                torch.save(model.state_dict(), os.path.join(model_save_root, f\"m1_bigger_v2__{cur_time}__{mean_val_loss:.5f}__greed_acc_{greedy_accuracy:.5f}__{model_name_postfix}__epoch_i_{epoch_i}.pt\"))\n","                print(f\"Greedy accuracy = {greedy_accuracy}\")\n","                print('Новая лучшая модель!')\n","            elif epoch_i - best_epoch_i > early_stopping_patience:\n","                print('Модель не улучшилась за последние {} эпох, прекращаем обучение'.format(\n","                    early_stopping_patience))\n","                break\n","\n","            if lr_scheduler is not None:\n","                lr_scheduler.step(mean_val_loss)\n","\n","            print()\n","        except KeyboardInterrupt:\n","            print('Досрочно остановлено пользователем')\n","            break\n","        except Exception as ex:\n","            print('Ошибка при обучении: {}\\n{}'.format(ex, traceback.format_exc()))\n","            break"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["EXPERIMENT_NAME = f\"m1_bigger_model__{GRID_NAME}__from_random_weights__batch__{BATCH_SIZE}/SEED_{RANDOM_SEED}__run1\"\n","TENSORBOARD_LOG_PATH = f\"/kaggle/working/tensorboard_log/{EXPERIMENT_NAME}\"\n","\n","tb = SummaryWriter(TENSORBOARD_LOG_PATH)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["l2_reg_alpha =  0 #5e-5\n","label_smoothing=  0 #0.045\n","epoch_start = 0\n","\n","best_val_loss, best_model = train_eval_loop(\n","    transformer, train_dataset, val_dataset, cross_entropy_with_reshape, tb, epoch_start,\n","    lr=1e-4, epoch_n=10000, batch_size=BATCH_SIZE, collate_fn = collate_fn,\n","    device=device, early_stopping_patience=20, l2_reg_alpha=l2_reg_alpha,\n","    max_batches_per_epoch_train=2000,\n","    max_batches_per_epoch_val=1000,\n","    optimizer_ctor=None,\n","    lr_scheduler_ctor=lr_scheduler,\n","    shuffle_train=True,\n","    dataloader_workers_n=0,\n","    criterion_ignore_index = word_char_tokenizer.char_to_idx['<pad>'],\n","    model_name_postfix = f'{GRID_NAME}_l2_{l2_reg_alpha}_ls{label_smoothing}',\n","    model_save_root = \"../..\",\n","    label_smoothing=label_smoothing,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Эпоха должна длиться 16 минут"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["import json\n","from typing import Optional, List, Tuple, Dict, Set\n","import array\n","\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset\n","from tqdm import tqdm\n","from torch.nn.utils.rnn import pad_sequence\n","\n","from tokenizers import CharLevelTokenizerv2, KeyboardTokenizerv1\n","\n","\n","class NeuroSwipeDatasetv3(Dataset):\n","    \"\"\"\n","    Dataset class for NeuroSwipe dataset.\n","\n","    The dataset uses all data from a given json in the same order as the json.\n","    There are separate json files for every grid.\n","\n","    Given a NeuroSwipeDatasetv3 object nsd, nsd[i] is a tuple:\n","    ((trajectory_features, k_key_tokens, decoder_in_char_seq), decoder_out_char_seq)\n","    \n","    ! Warning: refactoring planned so that the output is a dictionary.\n","\n","    WARNING:\n","    The class is in the process of refactoring. The padding will be done\n","    in DataLoaders collate_fn instead of __getitem__ method.\n","    Currently traj_pad_mask was removed. Word mask will be removed later.\n","    So nsd[i] = (\n","        (trajectory_features, k_key_tokens, decoder_in_char_seq, word_pad_mask), decoder_out_char_seq)\n","\n","    ! It seems reasonable for the dataset to always return grid_name as a\n","    dict property. We just won't use it in collate function.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 data_path: str,\n","                 gridname_to_grid: dict,\n","                 kb_tokenizer: KeyboardTokenizerv1,\n","                 word_tokenizer: CharLevelTokenizerv2,  # should contain max word len\n","                 include_time: bool = False,\n","                 include_velocities: bool = True,\n","                 include_accelerations: bool = True,\n","                 include_grid_name: bool = False,\n","                 has_target: bool = True,\n","                 has_one_grid_only: bool = True,\n","                 keyboard_selection_set: Optional[Set[str]] = None,\n","                 total: Optional[int] = None):\n","        \"\"\"\n","        Arguments:\n","        ----------\n","        data_path: str\n","            Path to the NeuroSwipe dataset in JSON format.\n","            A custom version of the dataset is used: \"grid\" property\n","            is replaced with \"grid_name\". The grid itself is stored in\n","            a separate gridname_to_grid dictionary.\n","            Dataset is a list of JSON lines. Each line is a dictionary\n","            with the following properties:\n","            - word (str): word that was typed. May be absent if has_target is False.\n","            - curve (dict): dictionary that contains the following properties:\n","                - x (List[int]): x coordinates of the swipe trajectory.\n","                - y (List[int]): y coordinates of the swipe trajectory.\n","                - t (List[int]): time in milliseconds from the beginning of the swipe.\n","                - grid_name (str): name of the keyboard grid.\n","\n","        gridname_to_grid: dict\n","            Dictionary that maps grid_name to grid.\n","            Grid is a dictionary that contains the following properties:\n","                - width (int): width of the keyboard in pixels.\n","                - height (int): height of the keyboard in pixels.\n","                - keys (List[dict]): list of keys. Each key is a dictionary\n","                    that contains the following properties:\n","                    - label (str): label of the key. May be absent if the key\n","                        is a special key (e.g. backspace).\n","                    - action (str): action of the key. May be absent if the key\n","                        is a character key (e.g. 'a', 'б', 'в').\n","                    - hitbox (dict): dictionary that contains the following properties:\n","                        - x (int): x coordinate of the top left corner of the key.\n","                        - y (int): y coordinate of the top left corner of the key.\n","                        - w (int): width of the key.\n","                        - h (int): height of the key.\n","            \n","        \n","        keyboard_selection_set: Optional[Set[str]]\n","            Set of keyboard key labels allowed. When looking\n","            for a key with the nearest to to trajectory point\n","            center coordinates we only consider keys with labels\n","            from this set.\n","            If None, all keys are allowed.\n","            Isn't used explicitly: only in is_allowed_label method.\n","\n","        \n","        total: Optional[int]\n","            Number of dataset elements. Is used only for progress bar.\n","\n","        \"\"\"\n","        if include_accelerations and not include_velocities:\n","            raise ValueError(\"Accelerations are supposed \\\n","                             to be an addition to velocities. Add velocities.\")\n","        \n","        if has_one_grid_only and len(gridname_to_grid) != 1:\n","            raise ValueError(f\"has_one_grid_only is True \\\n","                             but len(gridname_to_grid) != 1\")\n","\n","        self.include_velocities = include_velocities\n","        self.include_accelerations = include_accelerations\n","        self.include_time = include_time\n","        self.has_target = has_target\n","        self.include_grid_name = include_grid_name\n","        self._keyboard_selection_set = keyboard_selection_set\n","\n","        self.word_tokenizer = word_tokenizer\n","\n","        self._grid_name_to_grid = gridname_to_grid\n","\n","        self._nearest_kb_label_dict = (\n","            self._create_nearest_kb_label_dict(gridname_to_grid))\n","\n","        self.data_list = []\n","        self._set_data(data_path, gridname_to_grid,\n","                       kb_tokenizer, self.data_list, total = total)\n","\n","\n","    def is_allowed_label(self, label: str) -> bool:\n","        if self._keyboard_selection_set is None:\n","            return True\n","        return label in self._keyboard_selection_set\n","\n","\n","    def get_nearest_kb_label(self, x, y, grid_name, gridname_to_grid):\n","        \"\"\"\n","        Given coords on a keyboard (x, y) and its grid_name returns the nearest keyboard key\n","\n","        By default it uses an array assosiated with grid_name\n","        that stores the nearest key label for every possible coord pair.\n","\n","        If coords are outside of the keyboard boarders finds\n","        the nearest key by iterating over all keys.\n","        \"\"\"        \n","        grid = gridname_to_grid[grid_name]\n","        if x < 0 or x >= grid['width'] or y < 0 or y >= grid['height']:\n","            return self._get_kb_label_without_map(x, y, grid)\n","        else:\n","            return self._nearest_kb_label_dict[grid_name][x, y]\n","    \n","\n","    def _get_key_center(self, hitbox: Dict[str, int]) -> Tuple[int, int]:\n","        x = hitbox['x'] + hitbox['w'] / 2\n","        y = hitbox['y'] + hitbox['h'] / 2\n","        return x, y\n","    \n","    def _get_kb_label(self, key: dict) -> str:\n","        if 'label' in key:\n","            return key['label']\n","        if 'action' in key:\n","            return key['action']\n","        raise ValueError(\"Key has no label or action property\")\n","\n","\n","    def _get_kb_label_without_map(self, x, y, grid: dict) -> str:\n","        \"\"\"\n","        Returns label of the nearest key on the keyboard without using a map.\n","         \n","        Iterates over all keys and calculates the\n","        distance to (x, y) to find the nearest one.\n","        \"\"\"\n","        nearest_kb_label = None\n","        min_dist = float(\"inf\")\n","\n","        for key in grid['keys']:\n","            label = self._get_kb_label(key)\n","            \n","            if not self.is_allowed_label(label):\n","                continue\n","\n","            key_x, key_y = self._get_key_center(key['hitbox'])\n","            dist = (x - key_x)**2 + (y - key_y)**2\n","            if dist < min_dist:\n","                min_dist = dist\n","                nearest_kb_label = label \n","        return nearest_kb_label\n","\n","\n","    def _create_nearest_kb_label_dict(self, gridname_to_grid: dict\n","                                   ) -> Dict[str, np.array]:\n","        \"\"\"\n","        Creates a dict that maps grid_name to a map (np.array)\n","        from coordinates [x, y] to nearest key label.\n","        \"\"\"\n","        nearest_kb_label_dict = {}\n","        for grid_name, grid in gridname_to_grid.items():\n","            nearest_kb_label_dict[grid_name] = self._get_coord_to_kb_label(grid)\n","        return nearest_kb_label_dict\n","    \n","\n","    def _get_coord_to_kb_label(self, grid: dict) -> np.array: # dtype = object\n","        coord_to_kb_label = np.zeros(\n","            (grid['width'], grid['height']), dtype=object)  # 1080 x 640 in our case\n","        coord_to_kb_label.fill('')\n","\n","        for key in grid['keys']:\n","            label = self._get_kb_label(key)\n","\n","            if not self.is_allowed_label(label):\n","                continue\n","\n","            x_left = key['hitbox']['x']\n","            x_right = x_left + key['hitbox']['w']\n","            y_top = key['hitbox']['y']\n","            y_bottom = y_top + key['hitbox']['h']\n","\n","            coord_to_kb_label[x_left:x_right, y_top:y_bottom] = label\n","\n","        for x in range(grid['width']):\n","            for y in range(grid['height']):\n","                if coord_to_kb_label[x, y] != '':\n","                    continue\n","                coord_to_kb_label[x, y] = self._get_kb_label_without_map(x, y, grid)\n","\n","        return coord_to_kb_label\n","            \n","\n","    def _set_data(self,\n","                  data_path: str,\n","                  gridname_to_grid: dict,\n","                  kb_tokenizer,\n","                  data_list: list,\n","                  total: Optional[int] = None):\n","        with open(data_path, \"r\", encoding=\"utf-8\") as json_file:\n","            for line in tqdm(json_file, total = total):\n","                data_list.append(self._get_data_from_json_line(line, gridname_to_grid, kb_tokenizer))\n","\n","\n","    def _get_dx_dt(self,\n","                   X: torch.tensor,\n","                   T: torch.tensor) -> List[float]:\n","        \"\"\"\n","        Calculates dx/dt for a list of x coordinates and a list of t coordinates.\n","\n","        Arguments:\n","        ----------\n","        X : torch.tensor\n","            x (position) coordinates.\n","        T : torch.tensor\n","            T[i] = time from the beginning of the swipe corresponding to X[i].\n","        len : int\n","            Length of the swipe trajectory. Indexes greater than len are ignored.\n","        \"\"\"\n","        dx_dt = torch.zeros_like(X)\n","        # dx_dt[1:-1] = (X[2:] - X[:-2]) / (T[2:] - T[:-2])\n","        dx_dt[1:len(X)-1] = (X[2:len(X)] - X[:len(X)-2]) / (T[2:len(X)] - T[:len(X)-2])\n","\n","        # Example:\n","        # x0 x1 x2 x3\n","        # t0 t1 t2 t3\n","        # dx_dt[0] = 0\n","        # dx_dt[1] = (x2 - x0) / (t2 - t0)\n","        # dx_dt[2] = (x3 - x1) / (t3 - t1)\n","        # dx_dt[3] = 0\n","\n","\n","        # if True in torch.isnan(dx_dt):\n","        #     print(dx_dt)\n","        #     raise ValueError(\"dx_dt contains NaNs\")\n","\n","        return dx_dt\n","    \n","\n","    def _get_data_from_json_line(self,\n","                                 line,\n","                                 gridname_to_grid,\n","                                 kb_tokenizer) -> Tuple[list, list, list, str]:\n","        \"\"\"\n","        Parses a JSON line and returns a dictionary with data.\n","        \"\"\"\n","        data = json.loads(line)\n","\n","        X = array.array('h', data['curve']['x'])\n","        Y = array.array('h', data['curve']['y'])\n","        T = array.array('h', data['curve']['t'])\n","\n","        grid_name = data['curve']['grid_name']   \n","\n","        kb_labels = [self.get_nearest_kb_label(x, y, grid_name, gridname_to_grid) for x, y in zip(X, Y)]\n","        kb_tokens = [kb_tokenizer.get_token(label) for label in kb_labels]\n","        kb_tokens = array.array('h', kb_tokens)\n","\n","        if not self.has_target:\n","            return X, Y, T, kb_tokens, grid_name\n","        else:\n","            word: str = data['word']\n","            return X, Y, T, kb_tokens, word, grid_name\n","\n","\n","    def __len__(self):\n","        return len(self.data_list)\n","    \n","\n","    def __getitem__(self, idx):\n","        if self.has_target:\n","            X_list, Y_list, T_list, kb_tokens, word, grid_name = self.data_list[idx]\n","        else:\n","            X_list, Y_list, T_list, kb_tokens, grid_name = self.data_list[idx]\n","\n","        X = torch.tensor(X_list, dtype=torch.float32)\n","        Y = torch.tensor(Y_list, dtype=torch.float32)\n","        T = torch.tensor(T_list, dtype=torch.float32)\n","\n","        xyt = torch.cat(\n","            (\n","                X.reshape(-1, 1),\n","                Y.reshape(-1, 1),\n","            ),\n","            axis = 1\n","        )\n","\n","        if self.include_time:\n","            xyt = torch.cat(\n","                (\n","                    xyt,\n","                    T.reshape(-1, 1)\n","                ),\n","                axis = 1\n","            )\n","\n","        if self.include_velocities:\n","            dx_dt = self._get_dx_dt(X, T)\n","            dy_dt = self._get_dx_dt(Y, T)\n","            xyt = torch.cat(\n","                [\n","                    xyt,\n","                    dx_dt.reshape(-1, 1),\n","                    dy_dt.reshape(-1, 1)\n","                ],\n","                axis = 1\n","            )\n","\n","        if self.include_accelerations:\n","            d2x_dt2 = self._get_dx_dt(dx_dt, T)\n","            d2y_dt2 = self._get_dx_dt(dy_dt, T)\n","            xyt = torch.cat(\n","                [\n","                    xyt,\n","                    d2x_dt2.reshape(-1, 1),\n","                    d2y_dt2.reshape(-1, 1)\n","                ],\n","                axis = 1\n","            )\n","        \n","        \n","        grid = self._grid_name_to_grid[grid_name]\n","        xyt[:len(X_list), 0] = xyt[:len(X_list), 0] / grid['width'] \n","        xyt[:len(Y_list), 1] = xyt[:len(X_list), 1] / grid['height']\n","        # Switch to this:\n","        # xyt[:, 0] = xyt[:, 0] / grid['width'] \n","        # xyt[:, 1] = xyt[:, 1] / grid['height']\n","\n","        kb_tokens = torch.tensor(kb_tokens, dtype=torch.int64)\n","\n","        decoder_out_char_seq = None\n","        decoder_in_char_seq = None\n","        word_mask = None\n","\n","        if self.has_target:\n","            # <sos>, token1, token2, ... token_n, <eos>\n","            token_seq: List[int] = self.word_tokenizer.encode(word)\n","            token_seq = torch.tensor(token_seq, dtype = torch.int64)\n","\n","            # model inputs and outputs are one token smaller than max_word,\n","            # Model inputs: <sos>, token1, ... token_n, <pad_0>, <pad_1>, ... <pad_k>\n","            # Model outputs:       token1, ... token_n, <EOS!>,  <pad_1>, ... <pad_k>\n","            decoder_seq_len = self.word_tokenizer.max_word_len - 1\n","\n","            \n","            word_mask = torch.ones(decoder_seq_len, dtype=torch.bool)\n","           \n","            # <sos> and full word are not masked;\n","            # <eos> and all <pad> are masked.\n","            word_mask[:len(word) + 1] = False \n","            \n","            # <sos>, token1, ... token_n\n","            decoder_in_char_seq = torch.full(\n","                (decoder_seq_len,),\n","                self.word_tokenizer.char_to_idx['<pad>'],\n","                dtype=torch.int64)\n","            decoder_in_char_seq[:len(word) + 1] = token_seq[:-1]\n","\n","            # token1, ... token_n, <eos>\n","            decoder_out_char_seq = torch.full(\n","                (decoder_seq_len,),\n","                self.word_tokenizer.char_to_idx['<pad>'],\n","                dtype=torch.int64)\n","            decoder_out_char_seq[:len(word) + 1] = token_seq[1:]\n","        \n","        if self.include_grid_name:\n","            return (xyt, kb_tokens, decoder_in_char_seq, word_mask), decoder_out_char_seq, grid_name\n","        \n","        return (xyt, kb_tokens, decoder_in_char_seq, word_mask), decoder_out_char_seq\n","\n","\n","\n","class NeuroSwipeGridSubset(Dataset):\n","    def __init__(self, dataset: Dataset, grid_name: str):\n","        self.dataset = dataset\n","        self.grid_name = grid_name\n","        self.grid_name_idxs = self._get_grid_name_idxs()\n","        \n","            \n","    def _get_grid_name_idxs(self):\n","        grid_name_idxs: list[int] = []\n","        for i, (x, y, grid_name) in enumerate(self.dataset):\n","            if grid_name == self.grid_name:\n","                grid_name_idxs.append(i)\n","        return grid_name_idxs\n","\n","    \n","    def __len__(self):\n","        return len(self.grid_name_idxs)\n","    \n","    def __getitem__(self, idx):\n","        return self.dataset[self.grid_name_idxs[idx]]\n","    \n","\n","\n","def collate_fn(batch: list):\n","    \"\"\"\n","    batch - list of tuples:\n","    ((traj_feats, kb_tokens, dec_in_char_seq, word_pad_mask), dec_out_char_seq)\n","    \"\"\"\n","    x, dec_out_char_seq = zip(*batch)\n","    (traj_feats_no_pad, kb_tokens_no_pad,\n","     dec_in_char_seq, word_pad_mask) = zip(*x)\n","\n","    # traj_feats[i].shape = (curve_len, n_coord_feats)\n","    traj_feats = pad_sequence(traj_feats_no_pad, batch_first=False)  # (curves_len, batch_size, n_coord_feats)\n","    # kb_tokens[i].shape = (curve_len,) \n","    kb_tokens = pad_sequence(kb_tokens_no_pad, batch_first=False)  # (curves_len, batch_size)\n","    \n","    dec_in_char_seq = torch.stack(dec_in_char_seq).transpose_(0, 1)  # (chars_seq_len - 1, batch_size)\n","    dec_out_char_seq = torch.stack(dec_out_char_seq).transpose_(0, 1)  # (chars_seq_len - 1, batch_size)\n","    word_pad_mask = torch.stack(word_pad_mask)\n","    \n","\n","    max_curve_len = traj_feats.shape[0]\n","\n","    traj_lens = torch.tensor([len(x) for x in traj_feats_no_pad])\n","\n","    # Берем матрицу c len(traj_lens) строками вида [0, 1, ... , max_curve_len - 1].\n","    # Каждый элемент i-ой строки сравниваем с длиной i-ой траектории.  Получится\n","    # матрица, где True только на позициях, больших, чем длина соответствующей траектории.\n","    # (batch_size, max_curve_len)    \n","    traj_pad_mask = torch.arange(max_curve_len).expand(len(traj_lens), max_curve_len) >= traj_lens.unsqueeze(1)\n","\n","    return (traj_feats, kb_tokens, dec_in_char_seq, traj_pad_mask, word_pad_mask), dec_out_char_seq"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 9416/9416 [00:01<00:00, 6017.32it/s]\n"]}],"source":["def get_grid(grid_name: str, grids_path: str) -> dict:\n","    with open(grids_path, \"r\", encoding=\"utf-8\") as f:\n","        return json.load(f)[grid_name]\n","\n","gridname_to_grid  = {GRID_NAME: get_grid(GRID_NAME, os.path.join(DATA_ROOT, \"gridname_to_grid.json\"))}\n","\n","DS_KWARGS = dict(\n","    include_time = False,\n","    include_velocities = True,\n","    include_accelerations = True,\n","    has_target=True,\n","    has_one_grid_only=True,\n","    include_grid_name=False,\n","    keyboard_selection_set=set(ALL_CYRILLIC_LETTERS_ALPHABET_ORD)\n",")\n","\n","\n","val_ds_old = NeuroSwipeDatasetv3(\n","        data_path=GRID_NAME_TO_DS_PATHS[GRID_NAME]['val'],\n","        gridname_to_grid = gridname_to_grid,\n","        kb_tokenizer=kb_tokenizer,\n","        word_tokenizer = word_char_tokenizer,\n","        total = 9_416,\n","        **DS_KWARGS\n","    )"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["9416it [00:42, 221.74it/s]\n"]}],"source":["for el1, el2 in tqdm(zip(val_dataset, val_ds_old)):\n","    (traj_feats, kb_tokens, dec_in), dec_out = el1\n","    (tf_o, kb_o, di_o, _), do_o = el2\n","\n","    assert torch.equal(traj_feats, tf_o)\n","    assert torch.equal(kb_tokens , kb_o)\n","    assert torch.equal(dec_in, di_o[:sum(di_o!=35)])\n","    assert torch.equal(dec_out, do_o[:sum(do_o!=35)])"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([36, 14,  1])"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["dec_in"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([36, 14,  1])"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30665,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
