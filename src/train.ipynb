{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"NOISE_RANGE = 15  # set to 0 to avoid augmentation","metadata":{"execution":{"iopub.status.busy":"2024-04-16T22:30:35.456467Z","iopub.execute_input":"2024-04-16T22:30:35.456862Z","iopub.status.idle":"2024-04-16T22:30:35.475277Z","shell.execute_reply.started":"2024-04-16T22:30:35.456832Z","shell.execute_reply":"2024-04-16T22:30:35.474159Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2","metadata":{"execution":{"iopub.status.busy":"2024-04-16T22:30:35.477058Z","iopub.execute_input":"2024-04-16T22:30:35.477805Z","iopub.status.idle":"2024-04-16T22:30:35.500088Z","shell.execute_reply.started":"2024-04-16T22:30:35.477764Z","shell.execute_reply":"2024-04-16T22:30:35.499009Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd /kaggle/working/yandex-cup-2023-ml-neuroswipe\n! git pull\n! git checkout pytorch-lightning\n\n%cd /kaggle/working/yandex-cup-2023-ml-neuroswipe/src","metadata":{"execution":{"iopub.status.busy":"2024-04-16T22:30:35.509288Z","iopub.execute_input":"2024-04-16T22:30:35.509613Z","iopub.status.idle":"2024-04-16T22:30:37.735432Z","shell.execute_reply.started":"2024-04-16T22:30:35.509586Z","shell.execute_reply":"2024-04-16T22:30:37.734345Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"/kaggle/working/yandex-cup-2023-ml-neuroswipe\nAlready up to date.\nM\t.dvc/config\nAlready on 'pytorch-lightning'\nYour branch is up to date with 'origin/pytorch-lightning'.\n/kaggle/working/yandex-cup-2023-ml-neuroswipe/src\n","output_type":"stream"}]},{"cell_type":"code","source":"# !zip -r src_uniform_int_noise.zip /kaggle/working/yandex-cup-2023-ml-neuroswipe/src","metadata":{"execution":{"iopub.status.busy":"2024-04-16T22:30:37.737519Z","iopub.execute_input":"2024-04-16T22:30:37.737823Z","iopub.status.idle":"2024-04-16T22:30:37.756989Z","shell.execute_reply.started":"2024-04-16T22:30:37.737795Z","shell.execute_reply":"2024-04-16T22:30:37.756017Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# !rm -r lightning_logs\n# !rm -r checkpoints\n# !rm -r checkpoint_epoch_end\n# !rm src_uniform_int_noise.zip","metadata":{"execution":{"iopub.status.busy":"2024-04-16T22:30:37.758244Z","iopub.execute_input":"2024-04-16T22:30:37.758559Z","iopub.status.idle":"2024-04-16T22:30:37.776916Z","shell.execute_reply.started":"2024-04-16T22:30:37.758536Z","shell.execute_reply":"2024-04-16T22:30:37.776003Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_grid(grid_name: str, grids_path: str) -> dict:\n    with open(grids_path, \"r\", encoding=\"utf-8\") as f:\n        return json.load(f)[grid_name]","metadata":{"execution":{"iopub.status.busy":"2024-04-16T22:30:37.779191Z","iopub.execute_input":"2024-04-16T22:30:37.779476Z","iopub.status.idle":"2024-04-16T22:30:37.797439Z","shell.execute_reply.started":"2024-04-16T22:30:37.779451Z","shell.execute_reply":"2024-04-16T22:30:37.796328Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"from typing import List, Dict, Tuple, Optional, Set, Iterable\n\nimport json\n\ndef get_gridname_to_out_of_bounds_coords_dict(\n        data_paths: List[str], gridname_to_wh: dict,\n        totals: Iterable[Optional[int]] = None\n        ) -> Dict[str, Set[Tuple[int, int]]]:\n    \"\"\"\n    Returns a dictionary with grid names as keys and lists of out of bounds coordinates as values.\n    \"\"\"\n    totals = totals or [None] * len(data_paths)\n    \n    gname_to_out_of_bounds = {gname: set() for gname in gridname_to_wh.keys()}\n\n    for data_path, total in zip(data_paths, totals):\n        with open(data_path, \"r\", encoding=\"utf-8\") as json_file:\n            for line in tqdm(json_file, total=total):\n                json_data = json.loads(line)\n                curve = json_data['curve']\n                grid_name = curve['grid_name']\n                w, h = gridname_to_wh[grid_name]\n                X, Y = curve['x'], curve['y']\n                out_of_bounds = set((x, y) for x, y in zip(X, Y) \n                                    if x < 0 or x >= w or y < 0 or y >= h)\n                gname_to_out_of_bounds[grid_name].update(out_of_bounds)\n    return gname_to_out_of_bounds","metadata":{"execution":{"iopub.status.busy":"2024-04-16T22:30:37.798692Z","iopub.execute_input":"2024-04-16T22:30:37.798943Z","iopub.status.idle":"2024-04-16T22:30:37.821790Z","shell.execute_reply.started":"2024-04-16T22:30:37.798921Z","shell.execute_reply":"2024-04-16T22:30:37.820888Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from typing import Iterable, Dict, Optional, Set, Tuple\n\n\ndef update_out_of_bounds_with_noise(\n    noise_min, noise_max,\n    gname_to_out_of_bounds, gridname_to_wh: dict,\n    )-> Dict[str, Set[Tuple[int, int]]]:\n    \n    assert noise_min <= 0\n    assert noise_max >= 0\n    \n    additional_out_of_bounds = {gname: set() for gname in gridname_to_wh.keys()}\n    \n    for gname in gname_to_out_of_bounds.keys():\n        w, h = gridname_to_wh[gname]\n        \n        for x, y in gname_to_out_of_bounds[gname]:\n            for i in range(noise_min, noise_max+1):\n                for j in range(noise_min, noise_max+1):\n                    if x+i < 0 or x+i >= w or y+j < 0 or y+j >=h: \n                        additional_out_of_bounds[gname].add((x+i, y+j))\n        \n        for x in range(noise_min, w+noise_max+1):\n            for y in range(noise_min, 0):\n                additional_out_of_bounds[gname].add((x, y))\n        \n        for x in range(noise_min, w+noise_max+1):\n            for y in range(h+1, h+noise_max+1):\n                additional_out_of_bounds[gname].add((x, y))\n        \n        for x in range(w, w+noise_max+1):\n            for y in range(0, h+1):\n                additional_out_of_bounds[gname].add((x, y))\n        \n        for x in range(noise_min, 0):\n            for y in range(0, h+1):\n                additional_out_of_bounds[gname].add((x, y))\n                \n        gname_to_out_of_bounds[gname].update(additional_out_of_bounds[gname])\n        \n    return gname_to_out_of_bounds\n        ","metadata":{"execution":{"iopub.status.busy":"2024-04-16T22:31:20.783389Z","iopub.execute_input":"2024-04-16T22:31:20.783766Z","iopub.status.idle":"2024-04-16T22:31:20.810927Z","shell.execute_reply.started":"2024-04-16T22:31:20.783736Z","shell.execute_reply":"2024-04-16T22:31:20.809929Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\nclass RandIntToTrajTransform:\n    def __init__(self, min_ = -3, max_ = 3) -> None:\n        self.min = min_\n        self.max = max_\n        \n    def __call__(self, data):\n        X, Y, T, grid_name, tgt_word = data\n        X = np.array(X, dtype = int) + np.random.randint(self.min, self.max, (len(X),))\n        Y = np.array(Y, dtype = int) + np.random.randint(self.min, self.max, (len(Y),))\n        return X, Y, T, grid_name, tgt_word\n    \nclass SequentialTransform:\n    def __init__(self, transforms) -> None:\n        self.transforms = transforms\n    \n    def __call__(self, data):\n        for transform in self.transforms:\n            data = transform(data)\n        return data","metadata":{"execution":{"iopub.status.busy":"2024-04-16T22:31:22.283196Z","iopub.execute_input":"2024-04-16T22:31:22.283947Z","iopub.status.idle":"2024-04-16T22:31:22.305695Z","shell.execute_reply.started":"2024-04-16T22:31:22.283915Z","shell.execute_reply":"2024-04-16T22:31:22.304745Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# commands:\n# python ./src/save_fully_transformed_ds.py --jsonl_path ./data/data_separated_grid/train__default_only_no_errors__2023_10_31__03_26_16.jsonl --output_path ./train_default_grid_no_errors__2023_10_31_ft__uint8 --vocab_path ./data/data_separated_grid/voc.txt --gridname_to_grid_path ./data/data_separated_grid/gridname_to_grid.json --n_workers 0\n# python ./src/save_fully_transformed_ds.py --jsonl_path ./data/data_separated_grid/train__extra_only_no_errors__2023_11_01__19_49_14.jsonl --output_path ./train_extra_no_errors_uint8_datalist.pt --vocab_path ./data/data_separated_grid/voc.txt --gridname_to_grid_path ./data/data_separated_grid/gridname_to_grid.json --n_workers 0\n\n\n# import sys; import os; sys.path.insert(1, os.path.join(os.getcwd(), \"src\"))\n\n\nimport pickle\nfrom abc import ABC, abstractmethod\nfrom math import ceil\nimport os\n\nimport argparse\nimport torch\nfrom tqdm import tqdm \n\nfrom dataset import CurveDatasetWithMultiProcInit\nfrom nearest_key_lookup import NearestKeyLookup, ExtendedNearestKeyLookup\nfrom ns_tokenizers import KeyboardTokenizerv1, CharLevelTokenizerv2\nfrom ns_tokenizers import ALL_CYRILLIC_LETTERS_ALPHABET_ORD\nfrom predict import get_grid_name_to_grid\n# from transforms import InitTransform, GetItemTransform\nfrom transforms import FullTransform, TrajFeatsKbTokensTgtWord_InitTransform\nfrom dataset import _get_data_from_json_line\n\n\n\n\n\nargs = argparse.Namespace(**{\n    'jsonl_path': '../data/data_separated_grid/train__default_only_no_errors__2023_10_31__03_26_16.jsonl',\n    'output_path': '../data/fully_transformed_datasets/torch_saved_data_list/train_default_no_errors_uint8_datalist__TrajFeats_KbTokens_TgtWord.pt',\n    'vocab_path': '../data/data_separated_grid/voc.txt',\n    'gridname_to_grid_path': '../data/data_separated_grid/gridname_to_grid.json',\n    'n_workers': 0\n})\n\n# has one grid only\ngridname_to_grid = get_grid_name_to_grid(args.gridname_to_grid_path)\n\n\n\n\ndata_paths = [\n    '../data/data_separated_grid/valid__in_train_format__default_only.jsonl',\n    '../data/data_separated_grid/train__default_only_no_errors__2023_10_31__03_26_16.jsonl'\n]\n\n\ntotals = [None, 6_000_000]\n\ngname_to_wh = {\n    gname: (grid['width'], grid['height']) \n    for gname, grid in gridname_to_grid.items()\n}\n\n\nprint(\"Accumulating out-of-bounds coordinates...\")\ngname_to_out_of_bounds = get_gridname_to_out_of_bounds_coords_dict(\n    data_paths, gname_to_wh, totals=totals\n)\n\nprint(\"augmenting gname_to_out_of_bounds\")\ngname_to_out_of_bounds = update_out_of_bounds_with_noise(\n    noise_min = -NOISE_RANGE, noise_max=NOISE_RANGE+1,\n    gname_to_out_of_bounds = gname_to_out_of_bounds, gridname_to_wh = gname_to_wh,\n)\n\n\n\nprint(\"Creating ExtendedNearestKeyLookups...\")\ngridname_to_nkl = {\n    gname: ExtendedNearestKeyLookup(grid, ALL_CYRILLIC_LETTERS_ALPHABET_ORD, gname_to_out_of_bounds[gname])\n    for gname, grid in gridname_to_grid.items()\n}\n\n\n\n\n\nkb_tokenizer = KeyboardTokenizerv1()\nword_tokenizer = CharLevelTokenizerv2(args.vocab_path)\n\n\nfull_transform = FullTransform(\n    grid_name_to_nk_lookup=gridname_to_nkl,\n    grid_name_to_wh=gname_to_wh,\n    kb_tokenizer=kb_tokenizer,\n    word_tokenizer=word_tokenizer,\n    include_time=False,\n    include_velocities=True,\n    include_accelerations=True,\n    kb_tokens_dtype=torch.int32,\n    word_tokens_dtype=torch.int64\n)\n\n\n\ntrain_transform = None\nif NOISE_RANGE != 0:\n    augmentation_transform = RandIntToTrajTransform(-NOISE_RANGE, NOISE_RANGE + 1)\n    train_transform = SequentialTransform([augmentation_transform, full_transform])\nelse:\n    train_transform = full_transform\n    \nval_transform = full_transform\n\n\nprint(\"Calling CurveDatasetWithMultiProcInit.__init__ with full_transform...\")\n\n\n# jsonl_to_bins_on_disk(args.jsonl_path, 10_000, full_transform, args.output_path, total=5_237_584)\n\n\n\ntrain_dataset = CurveDatasetWithMultiProcInit(\n    data_path=args.jsonl_path,\n    store_gnames=False,\n    init_transform=None,\n    get_item_transform=train_transform,\n    n_workers=args.n_workers,\n    total=5_237_584 # 349172\n)\n\nval_dataset = CurveDatasetWithMultiProcInit(\n    data_path='../data/data_separated_grid/valid__in_train_format__default_only.jsonl',\n    store_gnames=False,\n    init_transform=None,\n    get_item_transform=val_transform,\n    n_workers=args.n_workers,\n    total=5_237_584 # 349172\n)\n# print(\"saving\")\n\n\n\n# move_list_to_disk_with_delete(10_000, args.output_path, ds.data_list)\n\n\n\n\n# torch.save(train_dataset.data_list, args.output_path)\n# # with open(args.output_path, 'wb') as f:\n# #     pickle.dump(ds.data_list, f)\n\n# # The script craches without error message sometimes and there's no way\n# # to know that it succeeded without this print\n# print(\"saved\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-16T22:31:25.525214Z","iopub.execute_input":"2024-04-16T22:31:25.525544Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Accumulating out-of-bounds coordinates...\n","output_type":"stream"},{"name":"stderr","text":"9416it [00:00, 22380.97it/s]\n 75%|███████▌  | 4522081/6000000 [03:17<01:04, 22947.98it/s]","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Я забыл, что сохранение занимает значительную часть времени. Но примерно через 5 секунд после начала сохранения (вывода `saving` на экран я запустил секундомер и знаю, что сохранение потребовало примерно 1 минуту 35 секунд\n\nЗначит, все, кроме сохранения потребовало 4 минуты 30 секунд\n\nВ принципе достаточно учитывать то, что выводит tqdm,  потому что nkl можно дешево создать и сохранить на диск\n\nТо есть создать extra dataset занимает 3 минуты 23 секунды","metadata":{}},{"cell_type":"markdown","source":"Посмотрим сколько времени нужно на загрузку датасета","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%time\n\n# import torch\n\n# from dataset import CurveDataset\n# from transforms import TrajFeatsKbTokensTgtWord_GetItemTransform\n\n# path = r'/kaggle/working/yandex-cup-2023-ml-neuroswipe/data/fully_transformed_datasets/torch_saved_data_list/train_default_no_errors_uint8_datalist__TrajFeats_KbTokens_TgtWord.pt'\n\n# data_list = torch.load(path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from transforms import TrajFeatsKbTokensTgtWord_GetItemTransform\n# from tokenizers import CharLevelTokenizerv2\n\n# VOC_PATH = '../data/data_separated_grid/voc.txt'\n\n# word_tokenizer = CharLevelTokenizerv2(VOC_PATH)\n# get_item_transform = TrajFeatsKbTokensTgtWord_GetItemTransform(\n#     word_tokenizer=word_tokenizer, tgt_word_dtype=torch.int64)\n\n# train_dataset = CurveDataset.from_data_list(data_list, get_item_transform=get_item_transform)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from save_fully_transformed_ds import BinsListStorage\n\n# load_list_from_bins = lambda path: BinsListStorage().read(path)\n\n# bins_path = '/kaggle/input/train-default-grid-no-errors-ft-uint8-bins/train_default_grid_no_errors__2023_10_31_ft__uint8'\n\n# load_list_from_bins(bins_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !python ./src/create_and_save_fully_transformed_ds.py --jsonl_path ./data/data_separated_grid/train__default_only_no_errors__2023_10_31__03_26_16.jsonl --output_path ./data/fully_transformed_datasets/train_default_grid_no_errors__2023_10_31_ft__int32.pt --vocab_path ./data/data_separated_grid/voc.txt --gridname_to_grid_path ./data/data_separated_grid/gridname_to_grid.json --n_workers 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !git clone https://github.com/proshian/yandex-cup-2023-ml-neuroswipe.git\n# %cd yandex-cup-2023-ml-neuroswipe","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install dvc --quiet\n# !pip install dvc_gdrive --quiet","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ! pip install gdown\n# ! python ./src/downloaders/download_weights.py","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working/yandex-cup-2023-ml-neuroswipe\n! git pull\n! git checkout pytorch-lightning","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working/yandex-cup-2023-ml-neuroswipe/src","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"############# Script arguments emulation #############\n\nGRID_NAME = \"default\"\nTRAIN_BATCH_SIZE = 256\nIN_KAGGLE = False\nRANDOM_SEED = 12\n\nDATA_ROOT = \"../data/data_separated_grid\"\nMODELS_DIR = \"../data/trained_models/m1\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport json\nimport typing as tp\nimport traceback\nfrom datetime import datetime\nimport copy\n\nimport torch\n# import torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom tqdm.notebook import tqdm\nimport numpy as np\nfrom torch.utils.tensorboard import SummaryWriter\n\n\nfrom model import SwipeCurveTransformer, get_m1_bigger_model\nfrom ns_tokenizers import CharLevelTokenizerv2, KeyboardTokenizerv1\nfrom ns_tokenizers import ALL_CYRILLIC_LETTERS_ALPHABET_ORD\nfrom dataset import CurveDataset, CollateFn\nfrom word_generators import GreedyGenerator\nfrom nearest_key_lookup import ExtendedNearestKeyLookup\nfrom transforms import KbTokens_InitTransform, KbTokens_GetItemTransform","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"################ Other constants ####################\nGRID_NAME_TO_DS_PATHS = {\n    \"extra\": {\n        \"train\": os.path.join(DATA_ROOT, \"train__extra_only_no_errors__2023_11_01__19_49_14.jsonl\"),\n        \"val\": os.path.join(DATA_ROOT, \"valid__in_train_format__extra_only.jsonl\")\n    },\n    \"default\": {\n        \"train\": os.path.join(DATA_ROOT, \"train__default_only_no_errors__2023_10_31__03_26_16.jsonl\"),\n        \"val\": os.path.join(DATA_ROOT, \"valid__in_train_format__default_only.jsonl\")\n    }\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GRID_NAME_TO_FT_DATALIST_PATHS = {\n    \"extra\": {\n        \"train\": None,\n        \"val\": None\n    },\n    \"default\": {\n        \"train\": None,\n        \"val\": None\n    }\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if IN_KAGGLE:\n#     DATA_ROOT = \"/kaggle/input/neuroswipe-defualt-only-v1\"\n#     MODELS_DIR = \"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def init_random_seed(value):\n    # random.seed(value)\n    np.random.seed(value)\n    torch.manual_seed(value)\n    torch.cuda.manual_seed(value)\n    # torch.backends.cudnn.deterministic = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_grid(grid_name: str, grids_path: str) -> dict:\n    with open(grids_path, \"r\", encoding=\"utf-8\") as f:\n        return json.load(f)[grid_name]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from typing import List, Dict, Tuple, Optional, Set\n\ndef get_gridname_to_out_of_bounds_coords_dict(\n        data_paths: List[str], gridname_to_wh: dict,\n        totals: tp.Iterable[Optional[int]] = None\n        ) -> Dict[str, Set[Tuple[int, int]]]:\n    \"\"\"\n    Returns a dictionary with grid names as keys and lists of out of bounds coordinates as values.\n    \"\"\"\n    totals = totals or [None] * len(data_paths)\n    \n    gname_to_out_of_bounds = {gname: set() for gname in gridname_to_wh.keys()}\n\n    for data_path, total in zip(data_paths, totals):\n        with open(data_path, \"r\", encoding=\"utf-8\") as json_file:\n            for line in tqdm(json_file, total=total):\n                json_data = json.loads(line)\n                curve = json_data['curve']\n                grid_name = curve['grid_name']\n                w, h = gridname_to_wh[grid_name]\n                X, Y = curve['x'], curve['y']\n                out_of_bounds = set((x, y) for x, y in zip(X, Y) \n                                    if x < 0 or x >= w or y < 0 or y >= h)\n                gname_to_out_of_bounds[grid_name].update(out_of_bounds)\n    return gname_to_out_of_bounds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_datasets(grid_name: str, grid_name_to_grid_path: str,\n                 data_paths: tp.Iterable[str], totals: tp.Iterable[tp.Optional[int]],\n                 nearest_key_candidates: tp.Set[str],\n                 kb_tokenizer: KeyboardTokenizerv1,\n                 word_char_tokenizer: CharLevelTokenizerv2\n                 ) -> List[CurveDataset]:\n    \n    gridname_to_grid  = {grid_name: get_grid(grid_name, grid_name_to_grid_path)}\n\n    gname_to_wh = {\n        gname: (grid['width'], grid['height']) \n        for gname, grid in gridname_to_grid.items()\n    }\n    \n    print(\"Accumulating out-of-bounds coordinates...\")\n    gname_to_out_of_bounds = get_gridname_to_out_of_bounds_coords_dict(\n        data_paths, gname_to_wh, totals=totals\n    )\n    \n    print(\"Creating ExtendedNearestKeyLookups...\")\n    gridname_to_nkl = {\n        gname: ExtendedNearestKeyLookup(grid, nearest_key_candidates, gname_to_out_of_bounds[gname])\n        for gname, grid in gridname_to_grid.items()\n    }\n    \n    \n    init_transform = KbTokens_InitTransform(\n        grid_name_to_nk_lookup=gridname_to_nkl,\n        kb_tokenizer=kb_tokenizer,\n    )\n\n    get_item_transform = KbTokens_GetItemTransform(\n        grid_name_to_wh=gname_to_wh,\n        word_tokenizer=word_char_tokenizer,\n        include_time=False,\n        include_velocities=True,\n        include_accelerations=True,\n    )\n    \n    print(\"Creating datasets...\")\n    datasets = []\n    for d_path, total in zip(data_paths, totals):\n        ds = CurveDataset(\n            data_path=d_path,\n            store_gnames = False,\n            init_transform=init_transform,\n            get_item_transform=get_item_transform,\n            total = total,\n        )\n        datasets.append(ds)\n    \n    return datasets","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"init_random_seed(RANDOM_SEED)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pickling the dataset would be great to not waste\n# around 20 minutes creating train_dataset.\n\nkb_tokenizer = KeyboardTokenizerv1()\nvoc_path=os.path.join(DATA_ROOT, \"voc.txt\")\nword_char_tokenizer = CharLevelTokenizerv2(voc_path)\n\n# data_paths = [\n#     GRID_NAME_TO_DS_PATHS[GRID_NAME]['train'],\n#     GRID_NAME_TO_DS_PATHS[GRID_NAME]['val']\n# ]\n# totals = [6_000_000, None]\n\n# data_paths = [\n#     GRID_NAME_TO_DS_PATHS[GRID_NAME]['val']\n# ]\n# totals = [None]\n\n\n# # train_dataset, val_dataset = get_datasets(\n# (val_dataset, ) = get_datasets(\n#     grid_name=GRID_NAME,\n#     grid_name_to_grid_path=os.path.join(DATA_ROOT, \"gridname_to_grid.json\"),\n#     data_paths = data_paths, totals = totals,\n#     nearest_key_candidates = ALL_CYRILLIC_LETTERS_ALPHABET_ORD,\n#     kb_tokenizer=kb_tokenizer,\n#     word_char_tokenizer=word_char_tokenizer,\n# )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%time\n\n# import pickle\n\n# try:\n#     with open(GRID_NAME_TO_FULLY_TRANSFORMED_DS_PATHS[GRID_NAME]['train'], 'rb') as f:\n#         data_list = pickle.load(f)\n# except e:\n#     print(e)\n\n# # train_dataset = CurveDataset.from_data_list(data_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformer = get_m1_bigger_model(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cross_entropy_with_reshape(pred, target, ignore_index=-100, label_smoothing=0.0):\n    \"\"\"\n    pred - BatchSize x TargetLen x VocabSize\n    target - BatchSize x TargetLen\n    \"\"\"\n    pred_flat = pred.view(-1, pred.shape[-1])  # BatchSize*TargetLen x VocabSize\n    target_flat = target.reshape(-1)  # BatchSize*TargetLen\n    return F.cross_entropy(pred_flat,\n                           target_flat,\n                           ignore_index=ignore_index,\n                           label_smoothing=label_smoothing)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lr_scheduler(optimizer):\n    return torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n                                                      patience=20,\n                                                      factor=0.5,\n                                                      verbose=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def move_all_to_device(x, device):\n#     if torch.is_tensor(x):\n#         return x.to(device)\n#     elif not isinstance(x, (list, tuple)):\n#         raise ValueError(f'Unexpected data type {type(x)}')\n#     new_x = []\n#     for el in x:\n#         if not torch.is_tensor(el):\n#             raise ValueError(f'Unexpected data type {type(el)}')\n#         new_x.append(el.to(device))\n#     return new_x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"collate_fn = CollateFn(\n    word_pad_idx = word_char_tokenizer.char_to_idx['<pad>'], batch_first = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Протестируем корректность collate_fn (вызывается неявно в DataLoader)\n\n# batch_size = 6\n\n\n# PAD_CHAR_TOKEN = word_char_tokenizer.char_to_idx[\"<pad>\"]\n\n\n# train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False,\n#                               num_workers=0, collate_fn=collate_fn)\n\n\n# dataset_els = [train_dataset[i] for i in range(batch_size)]\n# unproc_batch_x, unproc_batch_y = zip(*dataset_els)\n\n# batch_x, batch_y = next(iter(train_dataloader))\n\n\n# ############### Проверка корректности batch_y ###################\n# max_out_seq_len = max([len(y) for y in unproc_batch_y])\n\n# assert batch_y.shape == (max_out_seq_len, batch_size)\n\n\n# for i in range(batch_size):\n#     assert (batch_y[:len(unproc_batch_y[i]), i] == unproc_batch_y[i]).all()\n#     assert (batch_y[len(unproc_batch_y[i]):, i] == PAD_CHAR_TOKEN).all()\n\n# print(\"batch_y is correct\")\n\n\n\n# ############### Проверка корректности batch_x ###################\n# unproc_batch_traj_feats, unproc_batch_kb_tokens, unproc_batch_dec_in_char_seq = zip(*unproc_batch_x)\n\n# (traj_feats, kb_tokens, dec_in_char_seq, traj_pad_mask, word_pad_mask) = batch_x\n\n\n# # каждая сущность, полученная выше из unpoc_batch_x - это tuple длины batch_size.\n# # Например, unproc_batch_traj_feats[i] = train_dataset[i][0][0]\n\n# N_TRAJ_FEATS = 6\n# max_curve_len = max([el.shape[0] for el in unproc_batch_traj_feats]) \n\n# assert max_curve_len == max([el.shape[0] for el in unproc_batch_kb_tokens])\n\n# assert traj_feats.shape == (max_curve_len, batch_size, N_TRAJ_FEATS)\n# assert kb_tokens.shape == (max_curve_len, batch_size)\n# assert dec_in_char_seq.shape == (max_out_seq_len, batch_size)\n# assert traj_pad_mask.shape == (batch_size, max_curve_len)\n# assert word_pad_mask.shape == (batch_size, max_out_seq_len)\n\n\n# for i in range(batch_size):\n#     assert (traj_feats[:len(unproc_batch_traj_feats[i]), i] == unproc_batch_traj_feats[i]).all()\n#     assert (kb_tokens[:len(unproc_batch_kb_tokens[i]), i] == unproc_batch_kb_tokens[i]).all()\n\n#     assert (dec_in_char_seq[:len(unproc_batch_dec_in_char_seq[i]), i] == unproc_batch_dec_in_char_seq[i]).all()\n#     assert (dec_in_char_seq[len(unproc_batch_dec_in_char_seq[i]):, i] == PAD_CHAR_TOKEN).all()\n\n#     assert (traj_pad_mask[i, :len(unproc_batch_traj_feats[i])] == False).all()\n#     assert (traj_pad_mask[i, len(unproc_batch_traj_feats[i]):] == True).all()\n    \n#     assert (word_pad_mask[i, :len(unproc_batch_dec_in_char_seq[i])] == False).all()\n#     assert (word_pad_mask[i, len(unproc_batch_dec_in_char_seq[i]):] == True).all()\n\n# print(\"batch_x is correct\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from typing import List\n\n\ndef predict_greedy_raw(dataset,\n                       greedy_word_generator: GreedyGenerator,\n                       max_n_steps = 19, # длина самого длинного слова в валидационной выборке\n                      ) -> List[List[str]]:\n    \"\"\"\n    Creates predictions using greedy generation.\n\n    Supposed to be used with a dataset of a single grid\n    \n    Arguments:\n    ----------\n    dataset: NeuroSwipeDatasetv2\n    grid_name_to_greedy_generator: dict\n        Dict mapping grid names to GreedyGenerator objects.\n    \"\"\"\n    preds = [None] * len(dataset)\n\n    for data in tqdm(enumerate(dataset), total=len(dataset)):\n        i, ((xyt, kb_tokens, _), _) = data\n\n        pred = greedy_word_generator.generate_word_only(xyt, kb_tokens, max_n_steps)\n        pred = pred.removeprefix(\"<sos>\")\n        preds[i] = pred\n\n    return preds\n\n\ndef get_targets(dataset: CurveDataset) -> tp.List[str]:\n    targets = []\n    for _, target_tokens in dataset:\n        # Last token is <eos>.\n        target_str = word_char_tokenizer.decode(target_tokens[:-1])\n        targets.append(target_str)\n    return targets\n\n\ndef get_accuracy(preds, targets) -> float:\n    return sum(pred == target for pred, target \n               in zip(preds, targets)) / len(targets)\n\n\ndef get_greedy_generator_accuracy(val_dataset, model, \n                                  word_char_tokenizer, device) -> float:\n#     ! Лучше не гененрировать слово целиком, а продолжать побуквенно. \n#     Если буква не совпала сразу обрывать и говорить, \n#     что предсказание для этой кривой не совпало, а не гененировать все слово впустую\n    val_targets = get_targets(val_dataset)\n    greedy_generator = GreedyGenerator(model, word_char_tokenizer, device)\n    greedy_preds = predict_greedy_raw(val_dataset, greedy_generator)\n    return get_accuracy(greedy_preds, val_targets)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ###################### протестируем predict_greedy_raw ######################\n\n\n# # Главное теситровать не на случайных веах, потому что тогда будут генеироваться не короткие слова, а слова длиной max_seq_len\n\n\n# MODEL_TO_TEST_GREEDY_GEN__PATH = \"../data/trained_models_for_final_submit/m1_bigger/\" \\\n#     \"m1_bigger_v2__2023_11_11__14_29_37__0.13679_default_l2_0_ls0_switch_0.pt\"\n\n# # Leads to super slow inference.  I think it's due to \n# # high price of operations on small-amplitude floats.\n# # MODEL_TO_TEST_GREEDY_GEN__PATH = None\n\n\n# def test_greedy_generator(val_dataset, model_getter, model_weights, word_char_tokenizer, device) -> float:\n    \n#     model = model_getter(device, model_weights)\n\n#     return get_greedy_generator_accuracy(val_dataset, model, word_char_tokenizer, device)\n\n\n\n# test_greedy_generator(val_dataset, get_m1_bigger_model, MODEL_TO_TEST_GREEDY_GEN__PATH, word_char_tokenizer, device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# greedy_accuracy = get_greedy_generator_accuracy(val_dataset, model, word_char_tokenizer, device)\n# tb.add_scalar('greedy_accuracy/val', greedy_accuracy, epoch_i * n_train_examples_in_epoch)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODEL_NAME = \"m1_bigger\"\nUSE_AUGMENTATIONS_STR = f\"uniform_int_noise_{NOISE_RANGE}__\" if NOISE_RANGE else \"\"\nEXPERIMENT_NAME = f\"{MODEL_NAME}__{GRID_NAME}__{USE_AUGMENTATIONS_STR}from_random_weights__batch__{TRAIN_BATCH_SIZE}/SEED_{RANDOM_SEED}1\"\n# TENSORBOARD_LOG_PATH = f\"/kaggle/working/tensorboard_log/{EXPERIMENT_NAME}\"\n\n# tb = SummaryWriter(TENSORBOARD_LOG_PATH)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from tqdm import tqdm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import multiprocessing\n\nmultiprocessing.cpu_count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LOG_DIR = \"lightning_logs/\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install lightning","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_word_level_accuracy(y_true_batch: torch.Tensor, \n                            pred_batch: torch.Tensor, \n                            pad_token: int, \n                            mask: torch.Tensor) -> float:\n    # By default y_true.shape = pred.shape = (chars_seq_len, batch_size)\n    # So we have to transpose here or before calling\n\n    y_true_batch = y_true_batch.masked_fill(mask, pad_token)\n    pred_batch = pred_batch.masked_fill(mask, pad_token)\n    equality_results = torch.all(torch.eq(y_true_batch, pred_batch), dim = 1)\n        \n    return float(equality_results.sum() / len(equality_results))\n\n\ndecode_batch = lambda seq_batch, tokenizer: [tokenizer.decode(seq) for seq in seq_batch]\n\n\ndef get_word_level_metric(metric_fn,\n                          y_true_batch: torch.Tensor, \n                          pred_batch: torch.Tensor, \n                          tokenizer,\n                          mask: torch.Tensor) -> float:\n    \n    y_true_batch.masked_fill_(mask, tokenizer.char_to_idx['<pad>'])\n    pred_batch.masked_fill_(mask, tokenizer.char_to_idx['<pad>'])\n        \n    y_true_batch = decode_batch(y_true_batch, word_char_tokenizer)\n    pred_batch = decode_batch(pred_batch, word_char_tokenizer)\n    \n    return metric_fn(y_true_batch, pred_batch)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"######  testing get_word_level_accuracy, get_word_level_metric\nfrom sklearn.metrics import f1_score, accuracy_score\nimport torch\n\nbatch_size = 10\nseq_len = 5\ny_true__rand = torch.randint(0, 32, (batch_size, seq_len))\npred__rand = torch.randint(0, 32, (batch_size, seq_len))\npred__rand[:3] = y_true__rand[:3]\n\nmask = torch.zeros((batch_size, seq_len), dtype = torch.bool)\nmask[:, :-3] = True\n\nprint(\n    get_word_level_accuracy(\n        y_true__rand, pred__rand, pad_token = -1, mask = mask)\n)\n\nprint(\n    get_word_level_metric(accuracy_score, y_true__rand, pred__rand,\n                      word_char_tokenizer, mask = mask)\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torchmetrics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from lightning import LightningModule\nfrom lightning import Trainer\nfrom lightning.pytorch.callbacks.early_stopping import EarlyStopping\nfrom lightning.pytorch.callbacks import ModelCheckpoint\nfrom lightning.pytorch import loggers as pl_loggers\nimport torchmetrics\n\n\nfrom model import MODEL_GETTERS_DICT\n\n# ! Make sure:\n# * Add metrics\n\n#! Maybe store:\n# * batch_size\n# * early_stopping_patience\n\n#! Maybe:\n# * Checpointing by condition: if model improved on val_loss and val_loss < max_val_loss_to_save\n\n\nclass LitNeuroswipeModel(LightningModule):\n    def __init__(self, model_name: str, criterion, \n                 num_classes: int,\n                 train_batch_size: int = None,\n                 criterion_ignore_index: int = -100, optim_kwargs = None, \n                 optimizer_ctor=None, lr_scheduler_ctor=None, label_smoothing=0.0,\n                 ) -> None:\n        super().__init__()\n\n        self.optim_kwargs = optim_kwargs or dict(lr=1e-4, weight_decay=0)\n        \n        self.model_name = model_name\n        self.train_batch_size = train_batch_size\n        self.label_smoothing = label_smoothing\n        self.criterion_ignore_index = criterion_ignore_index\n\n        self.optimizer_ctor = optimizer_ctor\n        self.lr_scheduler_ctor = lr_scheduler_ctor\n\n        self.model = MODEL_GETTERS_DICT[model_name]()\n        self.criterion = criterion\n        \n        self.train_token_acc = torchmetrics.classification.Accuracy(\n            task=\"multiclass\", num_classes=num_classes, ignore_index=criterion_ignore_index)\n        self.val_token_acc = torchmetrics.classification.Accuracy(\n            task=\"multiclass\", num_classes=num_classes, ignore_index=criterion_ignore_index)\n        self.train_token_f1 = torchmetrics.classification.F1Score(\n            task=\"multiclass\", num_classes=num_classes, ignore_index=criterion_ignore_index)\n        self.val_token_f1 = torchmetrics.classification.F1Score(\n            task=\"multiclass\", num_classes=num_classes, ignore_index=criterion_ignore_index)\n\n    def forward(self, x, kb_tokens, y, x_pad_mask, y_pad_mask):\n        x_encoded = self.model.encode(x, kb_tokens, x_pad_mask)\n        return self.model.decode(x_encoded, y, x_pad_mask, y_pad_mask)\n    \n    def configure_optimizers(self):\n        optimizer = self.optimizer_ctor(self.parameters(), **self.optim_kwargs)\n        \n        optimizers_configuration = {'optimizer': optimizer}\n\n        if self.lr_scheduler_ctor:\n            lr_scheduler = self.lr_scheduler_ctor(optimizer)\n            optimizers_configuration['lr_scheduler'] = lr_scheduler\n            optimizers_configuration['monitor'] = 'val_loss'\n\n        return optimizers_configuration\n\n\n    def training_step(self, batch, batch_idx):\n        batch_x, batch_y = batch\n        \n        batch_size = batch_y.shape[-1]\n\n        # batch_x, batch_y = move_all_to_device(batch_x, batch_y, self.device)\n\n        # * batch_x is a Tuple of (curve_traj_feats, curve_kb_tokens,\n        #   decoder_in, curve_pad_mask, dec_seq_pad_mask).\n        # * batch_y is decoder_out.\n        \n        # preds.shape = (chars_seq_len, batch_size, n_classes)\n        \n        curve_traj_feats, curve_kb_tokens, ecoder_in, curve_pad_mask, dec_seq_pad_mask = batch_x\n\n        pred = self.forward(*batch_x)\n\n        loss = self.criterion(pred, batch_y, ignore_index=self.criterion_ignore_index,\n                              label_smoothing=self.label_smoothing)\n        \n        \n        argmax_pred = torch.argmax(pred, dim=2)\n        wl_acccuracy = get_word_level_accuracy(\n            argmax_pred.T, batch_y.T, pad_token = self.criterion_ignore_index, mask = dec_seq_pad_mask)\n        \n        \n        flat_y = batch_y.reshape(-1)\n        n_classes = pred.shape[-1]\n        flat_preds = pred.reshape(-1, n_classes)\n        \n        self.train_token_acc(flat_preds, flat_y)\n        self.log('train_token_level_accuracy', self.train_token_acc, on_step=True, on_epoch=False)\n        \n        self.train_token_f1(flat_preds, flat_y)\n        self.log('train_token_level_f1', self.train_token_f1, on_step=True, on_epoch=False)\n        \n        \n        self.log(\"train_word_level_accuracy\", wl_acccuracy, on_step=True, on_epoch=True, \n                 prog_bar=True, logger=True, batch_size = batch_size)\n        \n        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, \n                 prog_bar=True, logger=True, batch_size = batch_size)\n\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        batch_x, batch_y = batch\n        batch_size = batch_y.shape[-1]\n        # batch_x, batch_y = move_all_to_device(batch_x, batch_y, self.device)\n        curve_traj_feats, curve_kb_tokens, ecoder_in, curve_pad_mask, dec_seq_pad_mask = batch_x\n        pred = self.forward(*batch_x)\n        loss = self.criterion(pred, batch_y, ignore_index=self.criterion_ignore_index,\n                              label_smoothing=self.label_smoothing)\n        argmax_pred = torch.argmax(pred, dim=2)\n        wl_acccuracy = get_word_level_accuracy(\n            argmax_pred.T, batch_y.T, pad_token = self.criterion_ignore_index, mask = dec_seq_pad_mask)\n        \n        \n        flat_y = batch_y.reshape(-1)\n        n_classes = pred.shape[-1]\n        flat_preds = pred.reshape(-1, n_classes)\n        \n        \n        self.val_token_acc(flat_preds, flat_y)\n        self.log('val_token_level_accuracy', self.train_token_acc, on_step=False, on_epoch=True)\n        \n        self.val_token_f1(flat_preds, flat_y)\n        self.log('val_token_level_f1', self.train_token_f1, on_step=False, on_epoch=True)\n        \n        \n        \n        self.log(\"val_word_level_accuracy\", wl_acccuracy, on_step=False, on_epoch=True, \n                 prog_bar=True, logger=True, batch_size = batch_size)\n        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, \n                 logger=True, batch_size = batch_size)\n        return loss\n\n\ntb_logger = pl_loggers.TensorBoardLogger(save_dir=LOG_DIR, name=EXPERIMENT_NAME)\n\nearly_stopping_cb = EarlyStopping(\n    monitor='val_loss', mode = 'min', patience=25)\n\nmodel_checkpoint_cb = ModelCheckpoint(\n    monitor='val_loss', mode = 'min', save_top_k=50, \n    dirpath='checkpoints/', filename=f'{MODEL_NAME}-{GRID_NAME}--' + '{epoch}-{val_loss:.2f}-{val_word_level_accuracy:.2f}')\n\n# It's more reliable to continue training from epoch-end-checkpoints\nmodel_checkpoint_on_train_epoch_end = ModelCheckpoint(\n    save_on_train_epoch_end = True, dirpath='checkpoint_epoch_end/', \n    save_top_k=-1,\n    filename=f'{MODEL_NAME}-{GRID_NAME}--' + '{epoch}-{val_loss:.2f}-{val_word_level_accuracy:.2f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ls yandex-cup-2023-ml-neuroswipe/src/checkpoints","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataloader_workers_n = 4\n\nVAL_BATCH_SIZE = 512\n\ntrain_loader = DataLoader(\n    train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True,\n    num_workers=dataloader_workers_n, persistent_workers = True, \n    collate_fn=collate_fn)\n\nval_loader = DataLoader(val_dataset, batch_size=VAL_BATCH_SIZE, shuffle=False,\n                        num_workers=dataloader_workers_n, persistent_workers = True, \n                        collate_fn=collate_fn)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from lightning.pytorch.callbacks import Callback\n\nclass EmptyCudaCacheCallback(Callback):\n    def on_train_epoch_end(self, trainer, pl_module):\n        torch.cuda.empty_cache()\n        \nepmty_cuda_cache_cb = EmptyCudaCacheCallback()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ! Word-level accuracy of greedy search results and in-training-predictions are equal.\n# ! Thus GreedyAccuracyCallback doesn't make sence and should be deleted.\n\n# ! However if we would count char-level metrics,\n# ! greedy search results and in-training-predictions would be different\n\nclass GreedyAccuracyCallback(Callback):\n    def __init__(self, each_n_steps: int, val_dataset, word_char_tokenizer, logger):\n        self.each_n_steps = each_n_steps\n        self.val_dataset = val_dataset\n        self.word_char_tokenizer = word_char_tokenizer\n        self.logger = logger\n\n    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_index):\n        device = next(pl_module.parameters()).device\n        \n        if (pl_module.global_step + 1) % self.each_n_steps == 0:\n            greedy_accuracy = get_greedy_generator_accuracy(\n                val_dataset, pl_module.model, word_char_tokenizer, device)\n            self.logger.log_metrics({\"greedy_val_accuracy\": greedy_accuracy}, step = pl_module.global_step)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"greedy_acc_callback = GreedyAccuracyCallback(\n    each_n_steps = 9000, val_dataset=val_dataset, \n    word_char_tokenizer=word_char_tokenizer, logger = tb_logger)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_smoothing = 0.045\n\n\npl_model = LitNeuroswipeModel(\n    model_name = MODEL_NAME, criterion = cross_entropy_with_reshape, \n    num_classes = 35,  # = len(word_char_tokenizer.idx_to_char) - len(['<pad>', '<unk>']) = 37 - 2\n    train_batch_size = TRAIN_BATCH_SIZE,\n    criterion_ignore_index = word_char_tokenizer.char_to_idx['<pad>'], \n    optim_kwargs = dict(lr=1e-4, weight_decay=0), \n    optimizer_ctor=torch.optim.Adam, lr_scheduler_ctor=lr_scheduler, label_smoothing=0.045,\n)\n\ntrainer = Trainer(\n#     limit_train_batches = 400,  # for validating code before actual training\n    log_every_n_steps = 100,\n    num_sanity_val_steps=1,\n    accelerator = 'gpu',\n    max_epochs=1000,\n    callbacks=[\n        early_stopping_cb, model_checkpoint_cb, \n        model_checkpoint_on_train_epoch_end, epmty_cuda_cache_cb,\n    ],\n    logger=tb_logger,\n    val_check_interval=3000,\n)\n\ntrainer.fit(pl_model, train_loader, val_loader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}