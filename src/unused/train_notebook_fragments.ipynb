{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def predict_greedy_raw(dataset,\n",
    "                       greedy_word_generator: GreedyGenerator,\n",
    "                       max_n_steps = 19, # длина самого длинного слова в валидационной выборке\n",
    "                      ) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Creates predictions using greedy generation.\n",
    "\n",
    "    Supposed to be used with a dataset of a single grid\n",
    "    \n",
    "    Arguments:\n",
    "    ----------\n",
    "    dataset: NeuroSwipeDatasetv2\n",
    "    grid_name_to_greedy_generator: dict\n",
    "        Dict mapping grid names to GreedyGenerator objects.\n",
    "    \"\"\"\n",
    "    preds = [None] * len(dataset)\n",
    "\n",
    "    for data in tqdm(enumerate(dataset), total=len(dataset)):\n",
    "        i, ((xyt, kb_tokens, _), _) = data\n",
    "\n",
    "        pred = greedy_word_generator.generate_word_only(xyt, kb_tokens, max_n_steps)\n",
    "        pred = pred.removeprefix(\"<sos>\")\n",
    "        preds[i] = pred\n",
    "\n",
    "    return preds\n",
    "\n",
    "\n",
    "def get_targets(dataset: CurveDataset) -> tp.List[str]:\n",
    "    targets = []\n",
    "    for _, target_tokens in dataset:\n",
    "        # Last token is <eos>.\n",
    "        target_str = word_char_tokenizer.decode(target_tokens[:-1])\n",
    "        targets.append(target_str)\n",
    "    return targets\n",
    "\n",
    "\n",
    "def get_accuracy(preds, targets) -> float:\n",
    "    return sum(pred == target for pred, target \n",
    "               in zip(preds, targets)) / len(targets)\n",
    "\n",
    "\n",
    "def get_greedy_generator_accuracy(val_dataset, model, \n",
    "                                  word_char_tokenizer, device) -> float:\n",
    "#     ! Лучше не гененрировать слово целиком, а продолжать побуквенно. \n",
    "#     Если буква не совпала сразу обрывать и говорить, \n",
    "#     что предсказание для этой кривой не совпало, а не гененировать все слово впустую\n",
    "    val_targets = get_targets(val_dataset)\n",
    "    greedy_generator = GreedyGenerator(model, word_char_tokenizer, device)\n",
    "    greedy_preds = predict_greedy_raw(val_dataset, greedy_generator)\n",
    "    return get_accuracy(greedy_preds, val_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###################### протестируем predict_greedy_raw ######################\n",
    "\n",
    "\n",
    "# # Главное теситровать не на случайных веах, потому что тогда будут генеироваться не короткие слова, а слова длиной max_seq_len\n",
    "\n",
    "\n",
    "# MODEL_TO_TEST_GREEDY_GEN__PATH = \"../data/trained_models_for_final_submit/m1_bigger/\" \\\n",
    "#     \"m1_bigger_v2__2023_11_11__14_29_37__0.13679_default_l2_0_ls0_switch_0.pt\"\n",
    "\n",
    "# # Leads to super slow inference.  I think it's due to \n",
    "# # high price of operations on small-amplitude floats.\n",
    "# # MODEL_TO_TEST_GREEDY_GEN__PATH = None\n",
    "\n",
    "\n",
    "# def test_greedy_generator(val_dataset, model_getter, model_weights, word_char_tokenizer, device) -> float:\n",
    "    \n",
    "#     model = model_getter(device, model_weights)\n",
    "\n",
    "#     return get_greedy_generator_accuracy(val_dataset, model, word_char_tokenizer, device)\n",
    "\n",
    "\n",
    "\n",
    "# test_greedy_generator(val_dataset, get_m1_bigger_model, MODEL_TO_TEST_GREEDY_GEN__PATH, word_char_tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! Word-level accuracy of greedy search results and in-training-predictions are equal.\n",
    "# ! Thus GreedyAccuracyCallback doesn't make sence and should be deleted.\n",
    "\n",
    "# ! However if we would count char-level metrics,\n",
    "# ! greedy search results and in-training-predictions would be different\n",
    "\n",
    "class GreedyAccuracyCallback(Callback):\n",
    "    def __init__(self, each_n_steps: int, val_dataset, word_char_tokenizer, logger):\n",
    "        self.each_n_steps = each_n_steps\n",
    "        self.val_dataset = val_dataset\n",
    "        self.word_char_tokenizer = word_char_tokenizer\n",
    "        self.logger = logger\n",
    "\n",
    "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_index):\n",
    "        device = next(pl_module.parameters()).device\n",
    "        \n",
    "        if (pl_module.global_step + 1) % self.each_n_steps == 0:\n",
    "            greedy_accuracy = get_greedy_generator_accuracy(\n",
    "                val_dataset, pl_module.model, word_char_tokenizer, device)\n",
    "            self.logger.log_metrics({\"greedy_val_accuracy\": greedy_accuracy}, step = pl_module.global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_acc_callback = GreedyAccuracyCallback(\n",
    "    each_n_steps = 9000, val_dataset=val_dataset, \n",
    "    word_char_tokenizer=word_char_tokenizer, logger = tb_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Протестируем корректность collate_fn (вызывается неявно в DataLoader)\n",
    "\n",
    "# batch_size = 6\n",
    "\n",
    "\n",
    "# PAD_CHAR_TOKEN = word_char_tokenizer.char_to_idx[\"<pad>\"]\n",
    "\n",
    "\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False,\n",
    "#                               num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "# dataset_els = [train_dataset[i] for i in range(batch_size)]\n",
    "# unproc_batch_x, unproc_batch_y = zip(*dataset_els)\n",
    "\n",
    "# batch_x, batch_y = next(iter(train_dataloader))\n",
    "\n",
    "\n",
    "# ############### Проверка корректности batch_y ###################\n",
    "# max_out_seq_len = max([len(y) for y in unproc_batch_y])\n",
    "\n",
    "# assert batch_y.shape == (max_out_seq_len, batch_size)\n",
    "\n",
    "\n",
    "# for i in range(batch_size):\n",
    "#     assert (batch_y[:len(unproc_batch_y[i]), i] == unproc_batch_y[i]).all()\n",
    "#     assert (batch_y[len(unproc_batch_y[i]):, i] == PAD_CHAR_TOKEN).all()\n",
    "\n",
    "# print(\"batch_y is correct\")\n",
    "\n",
    "\n",
    "\n",
    "# ############### Проверка корректности batch_x ###################\n",
    "# unproc_batch_traj_feats, unproc_batch_kb_tokens, unproc_batch_dec_in_char_seq = zip(*unproc_batch_x)\n",
    "\n",
    "# (traj_feats, kb_tokens, dec_in_char_seq, traj_pad_mask, word_pad_mask) = batch_x\n",
    "\n",
    "\n",
    "# # каждая сущность, полученная выше из unpoc_batch_x - это tuple длины batch_size.\n",
    "# # Например, unproc_batch_traj_feats[i] = train_dataset[i][0][0]\n",
    "\n",
    "# N_TRAJ_FEATS = 6\n",
    "# max_curve_len = max([el.shape[0] for el in unproc_batch_traj_feats]) \n",
    "\n",
    "# assert max_curve_len == max([el.shape[0] for el in unproc_batch_kb_tokens])\n",
    "\n",
    "# assert traj_feats.shape == (max_curve_len, batch_size, N_TRAJ_FEATS)\n",
    "# assert kb_tokens.shape == (max_curve_len, batch_size)\n",
    "# assert dec_in_char_seq.shape == (max_out_seq_len, batch_size)\n",
    "# assert traj_pad_mask.shape == (batch_size, max_curve_len)\n",
    "# assert word_pad_mask.shape == (batch_size, max_out_seq_len)\n",
    "\n",
    "\n",
    "# for i in range(batch_size):\n",
    "#     assert (traj_feats[:len(unproc_batch_traj_feats[i]), i] == unproc_batch_traj_feats[i]).all()\n",
    "#     assert (kb_tokens[:len(unproc_batch_kb_tokens[i]), i] == unproc_batch_kb_tokens[i]).all()\n",
    "\n",
    "#     assert (dec_in_char_seq[:len(unproc_batch_dec_in_char_seq[i]), i] == unproc_batch_dec_in_char_seq[i]).all()\n",
    "#     assert (dec_in_char_seq[len(unproc_batch_dec_in_char_seq[i]):, i] == PAD_CHAR_TOKEN).all()\n",
    "\n",
    "#     assert (traj_pad_mask[i, :len(unproc_batch_traj_feats[i])] == False).all()\n",
    "#     assert (traj_pad_mask[i, len(unproc_batch_traj_feats[i]):] == True).all()\n",
    "    \n",
    "#     assert (word_pad_mask[i, :len(unproc_batch_dec_in_char_seq[i])] == False).all()\n",
    "#     assert (word_pad_mask[i, len(unproc_batch_dec_in_char_seq[i]):] == True).all()\n",
    "\n",
    "# print(\"batch_x is correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def move_all_to_device(x, device):\n",
    "#     if torch.is_tensor(x):\n",
    "#         return x.to(device)\n",
    "#     elif not isinstance(x, (list, tuple)):\n",
    "#         raise ValueError(f'Unexpected data type {type(x)}')\n",
    "#     new_x = []\n",
    "#     for el in x:\n",
    "#         if not torch.is_tensor(el):\n",
    "#             raise ValueError(f'Unexpected data type {type(el)}')\n",
    "#         new_x.append(el.to(device))\n",
    "#     return new_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TENSORBOARD_LOG_PATH = f\"/kaggle/working/tensorboard_log/{EXPERIMENT_NAME}\"\n",
    "\n",
    "# tb = SummaryWriter(TENSORBOARD_LOG_PATH)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
