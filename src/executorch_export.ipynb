{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting the model via executorch for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook exports the model to be used in an Android app via executorch runtime\n",
    "\n",
    "I created the app from a blank android project.\n",
    "\n",
    "To use the model in an app:\n",
    "1. Follow [official instructions](https://pytorch.org/executorch/0.5/getting-started-setup.html) for installing the **0.5** release of executorch. Note that I used 0.5 version! Make sure to use pybind xnnpack flag during requirements instalation: `./install_requirements.sh --pybind xnnpack`\n",
    "2. Follow [official instructions](https://pytorch.org/executorch/0.5/demo-apps-android.html) for buildig . Note that you only need instruction related to XNNPACK and don't need anything related to Qualcomm Hexagon NPU.\n",
    "    * So basically, You do the `Build the CMake target for the library with XNNPACK backend:` and `Build the Android extension:` under the `XNNPACK` header\n",
    "\n",
    "3. Put the model in `.pte` format exported in this notebook to the folder `{your-app-root-dir-name}/app/src/main/assets`\n",
    "\n",
    "4. Obtain the `executorch.aar` library and place it in app/libs/executorch.aar. The recommended way to do that is to download a prebuild library:\n",
    "\n",
    "   ```sh\n",
    "   # The ink is taken from official documentation: \n",
    "   # https://pytorch.org/executorch/0.5/android-prebuilt-library.html#using-prebuilt-libraries\n",
    "   mkdir -p app/libs\n",
    "   curl https://ossci-android.s3.amazonaws.com/executorch/release/v0.5.0-rc3/executorch.aar -o app/libs/executorch.aar\n",
    "   ```\n",
    "\n",
    "\n",
    "5. Add following to `build.gradle.kts`:\n",
    "\n",
    "   ``` groovy\n",
    "   dependencies {\n",
    "      implementation(files(\"libs/executorch.aar\"))\n",
    "      implementation(\"com.facebook.fbjni:fbjni:0.5.1\")\n",
    "      implementation(\"com.facebook.soloader:soloader:0.10.5\")\n",
    "   }\n",
    "   ```\n",
    "\n",
    "6. Copied BUCK file from the official example app (though I'm not sure that the BUCK file is needed)\n",
    "7. Maybe I forgot to mention some steps and maybe not all steps listed are nesessary 🙃\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "1. remove dropout from exported programs\n",
    "2. вероятно нужно сделать модули FullEncoder \n",
    "3. проверить, игнорируются поля модели, которые не используются в forward (например, есть ли разница между текущей инмплементацией `Decode` и имплементацией, где вся модель хранится как удинственное поле Decode) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Union, Tuple\n",
    "import os\n",
    "import array\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.export import export, ExportedProgram, Dim\n",
    "from executorch.exir import EdgeProgramManager, to_edge, to_edge_transform_and_lower\n",
    "from executorch.exir.backend.backend_api import LoweredBackendModule, to_backend\n",
    "from executorch.backends.xnnpack.partition.xnnpack_partitioner import XnnpackPartitioner\n",
    "\n",
    "from model import MODEL_GETTERS_DICT, EncoderDecoderTransformerLike\n",
    "from feature_extractors import get_val_transform\n",
    "from ns_tokenizers import CharLevelTokenizerv2, KeyboardTokenizerv1\n",
    "from word_generators_v2 import BeamGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_prefix(s: str, prefix: str) -> str:\n",
    "    if s.startswith(prefix):\n",
    "        s = s[len(prefix):]\n",
    "    return s\n",
    "\n",
    "\n",
    "def get_state_dict_from_checkpoint(ckpt: dict) -> Dict[str, torch.Tensor]:\n",
    "    return {remove_prefix(k, 'model.'): v for k, v in ckpt['state_dict'].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMAND LINE ARGUMENTS EMULATION\n",
    "\n",
    "MODEL_NAME = 'v3_nearest_and_traj_transformer_bigger'\n",
    "CHECKPOINT_ROOT_PATH = '../../checkpoints_for_executorch/my_nearest_features/'\n",
    "CHECKPOINT_PATH = os.path.join(CHECKPOINT_ROOT_PATH, 'v3_nearest_and_traj_transformer_bigger-default--epoch=73-val_loss=0.444-val_word_level_accuracy=0.872.ckpt')\n",
    "TRANSFORM_NAME =  \"traj_feats_and_nearest_key\"\n",
    "\n",
    "DATA_ROOT = '../data/data_preprocessed'\n",
    "\n",
    "GRIDNAME_TO_GRID_PATH = os.path.join(DATA_ROOT, \"gridname_to_grid.json\")\n",
    "voc_path=os.path.join(DATA_ROOT, \"voc.txt\")\n",
    "char_tokenizer = CharLevelTokenizerv2(voc_path)\n",
    "kb_tokenizer = KeyboardTokenizerv1()\n",
    "\n",
    "USE_TIME = False\n",
    "USE_VELOCITY = True\n",
    "USE_ACCELERATION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the trained model\n",
    "\n",
    "state_dict = get_state_dict_from_checkpoint(\n",
    "    torch.load(CHECKPOINT_PATH, map_location='cpu', weights_only=True))\n",
    "\n",
    "model: EncoderDecoderTransformerLike = MODEL_GETTERS_DICT[MODEL_NAME]().eval()\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing model input \n",
    "1. encoder_in\n",
    "2. decoder_in, decoder_out_target, encoded_swipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prepare_encoder_input(encoder_in: Union[Tensor, Tuple[Tensor, Tensor]], \n",
    "                           device: str, batch_first: bool\n",
    "                           ) -> Tuple[Tensor, Tensor]:\n",
    "    is_tensor = None\n",
    "    if isinstance(encoder_in, Tensor):\n",
    "        is_tensor = True\n",
    "        encoder_in = [encoder_in]\n",
    "    else:\n",
    "        is_tensor = False\n",
    "\n",
    "    encoder_in = [el.unsqueeze(0) if batch_first else el.unsqueeze(1) for el in encoder_in]\n",
    "    encoder_in = [el.to(device) for el in encoder_in]\n",
    "\n",
    "    return encoder_in[0] if is_tensor else encoder_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATASET_ITEM_EXAMPLE = (\n",
    "    array.array('h', [567, 567, 507, 424, 380, 348, 337, 332, 330, 329, 327, 326, 326]),\n",
    "    array.array('h', [66, 66, 101, 161, 196, 230, 240, 245, 247, 249, 251, 251, 251]),\n",
    "    array.array('h', [0, 3, 24, 52, 75, 90, 106, 129, 145, 161, 177, 195, 209]),\n",
    "    'default',\n",
    "    'на')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = get_val_transform(\n",
    "    gridname_to_grid_path=GRIDNAME_TO_GRID_PATH,\n",
    "    grid_names=['default'],\n",
    "    transform_name=TRANSFORM_NAME,\n",
    "    char_tokenizer=char_tokenizer,\n",
    "    uniform_noise_range=0,\n",
    "    include_time=USE_TIME,\n",
    "    include_velocities=USE_VELOCITY,\n",
    "    include_accelerations=USE_ACCELERATION,\n",
    "    dist_weights_func=None,  # Fill if weighted version is used\n",
    "    ds_paths_list=[],\n",
    ")\n",
    "\n",
    "\n",
    "(encoder_in, decoder_in), decoder_out_target = transform(RAW_DATASET_ITEM_EXAMPLE)\n",
    "encoder_in = _prepare_encoder_input(encoder_in, 'cpu', batch_first=False)\n",
    "if isinstance(encoder_in, list):\n",
    "    encoder_in = tuple(encoder_in)\n",
    "decoder_in = decoder_in.unsqueeze(1).to(torch.int32)\n",
    "\n",
    "encoded = model.encode(encoder_in, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative: Manual data creation\n",
    "In this case all inputs are torch.ones. We can allso generate random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get data manually\n",
    "# SWIPE_LENGTH = 13\n",
    "# BATCH_SIZE = 1\n",
    "# NUM_TRAJ_FEATS = 6\n",
    "# OUT_SEQ_LEN = 3\n",
    "\n",
    "# sample_kb_key_ids = torch.ones((SWIPE_LENGTH, BATCH_SIZE), dtype=torch.int32)\n",
    "# sample_traj_feats = torch.ones((SWIPE_LENGTH, BATCH_SIZE, NUM_TRAJ_FEATS), dtype=torch.float32)\n",
    "# encoder_in = (sample_traj_feats, sample_kb_key_ids)\n",
    "# decoder_in = torch.ones((OUT_SEQ_LEN, BATCH_SIZE), dtype=torch.int32)\n",
    "# encoded = model.encode(\n",
    "#     encoder_in, \n",
    "#     None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXECUTORCH_MODELS_DIR = \"../results/executorch_models/\"\n",
    "os.makedirs(EXECUTORCH_MODELS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encode(torch.nn.Module):\n",
    "    def __init__(self, model) -> None:\n",
    "        super().__init__()\n",
    "        self.enc_in_emb_model = model.enc_in_emb_model\n",
    "        self.encoder = model.encoder\n",
    "\n",
    "    def forward(self, encoder_in):\n",
    "        x = self.enc_in_emb_model(encoder_in)\n",
    "        result = self.encoder(x, src_key_padding_mask = None)\n",
    "        return result\n",
    "\n",
    "\n",
    "class Decode(torch.nn.Module):\n",
    "    def __init__(self, model) -> None:\n",
    "        super().__init__()\n",
    "        self.dec_in_emb_model = model.dec_in_emb_model\n",
    "        self.decoder = model.decoder\n",
    "        self._get_mask = model._get_mask\n",
    "        self.out = model.out\n",
    "\n",
    "    def forward(self, decoder_in, x_encoded):\n",
    "        y = self.dec_in_emb_model(decoder_in)\n",
    "        tgt_mask = self._get_mask(y.size(0))  # = self.causal_mask[y.size(0):, y.size(0):]\n",
    "        dec_out = self.decoder(\n",
    "            y, x_encoded, tgt_mask=tgt_mask, \n",
    "            memory_key_padding_mask=None, \n",
    "            tgt_key_padding_mask=None,\n",
    "            tgt_is_causal=True)\n",
    "        return self.out(dec_out)\n",
    "    \n",
    "\n",
    "MAX_SWIPE_LEN = 299\n",
    "MAX_WORD_LEN = 35\n",
    "dim_swipe_seq = Dim(\"dim_swipe_seq\", min=1, max=MAX_SWIPE_LEN)\n",
    "dim_char_seq = Dim(\"dim_char_seq\", min=1, max=MAX_WORD_LEN)\n",
    "\n",
    "encoder_dynamic_shapes = {\"encoder_in\": ({0: dim_swipe_seq}, {0: dim_swipe_seq})}\n",
    "decoder_dynamic_shapes = {\n",
    "    \"x_encoded\": {0: dim_swipe_seq},\n",
    "    \"decoder_in\": {0: dim_char_seq}\n",
    "}\n",
    "\n",
    "aten_encode: ExportedProgram = export(Encode(model).eval(), (encoder_in,), dynamic_shapes=encoder_dynamic_shapes)\n",
    "aten_decode: ExportedProgram = export(Decode(model).eval(), (decoder_in, encoded), dynamic_shapes=decoder_dynamic_shapes)\n",
    "\n",
    "edge_xnnpack: EdgeProgramManager = to_edge_transform_and_lower(\n",
    "    {\"encode\": aten_encode, \"decode\": aten_decode},\n",
    "    partitioner=[XnnpackPartitioner()],\n",
    ")\n",
    "\n",
    "exec_prog_xnnpack = edge_xnnpack.to_executorch()\n",
    "\n",
    "with open(os.path.join(EXECUTORCH_MODELS_DIR, \"xnnpack_my_nearest_feats.pte\"), \"wb\") as file:\n",
    "    exec_prog_xnnpack.write_to_file(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_program = to_edge({\"encode\": aten_encode, \"decode\": aten_decode})\n",
    "\n",
    "executorch_program = edge_program.to_executorch()\n",
    "\n",
    "with open(os.path.join(EXECUTORCH_MODELS_DIR, \"raw_my_nearest_feats.pte\"), \"wb\") as file:\n",
    "    file.write(executorch_program.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(aten_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(exec_prog_xnnpack.exported_program('encode'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(exec_prog_xnnpack.exported_program('encode').graph_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exec_prog_xnnpack.exported_program('encode').module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's create tests to make sure that the model in the app creates same output as a pytorch version\n",
    "\n",
    "We'll save the transformed validation data to files (encoder_in, decoder_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data manually\n",
    "SWIPE_LENGTH = 13\n",
    "BATCH_SIZE = 1\n",
    "NUM_TRAJ_FEATS = 6\n",
    "OUT_SEQ_LEN = 3\n",
    "\n",
    "sample_kb_key_ids = torch.ones((SWIPE_LENGTH, BATCH_SIZE), dtype=torch.int64)\n",
    "sample_traj_feats = torch.ones((SWIPE_LENGTH, BATCH_SIZE, NUM_TRAJ_FEATS), dtype=torch.float32)\n",
    "encoder_in = (sample_traj_feats, sample_kb_key_ids)\n",
    "decoder_in = torch.ones((OUT_SEQ_LEN, BATCH_SIZE), dtype=torch.int64)\n",
    "\n",
    "encoded = model.encode(\n",
    "    encoder_in, \n",
    "    None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = model.decode(decoder_in, encoded, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_dict(tensor: torch.Tensor) -> dict:\n",
    "    return {\n",
    "        'data': tensor.reshape(-1).tolist(),\n",
    "        'shape': tuple(tensor.shape)\n",
    "    }\n",
    "\n",
    "def model_input_to_dict(encoder_in, decoder_in):\n",
    "    return {\n",
    "        'encoder_in': [tensor_to_dict(encoder_in_i) for encoder_in_i in encoder_in],\n",
    "        'decoder_in': tensor_to_dict(decoder_in)\n",
    "    }\n",
    "\n",
    "def model_output_to_dict(encoder_out, decoder_out):\n",
    "    return {\n",
    "        'encoder_out': tensor_to_dict(encoder_out),\n",
    "        'decoder_out': tensor_to_dict(decoder_out)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_input.json', 'w') as f:\n",
    "    json.dump(model_input_to_dict(encoder_in, decoder_in), f)\n",
    "\n",
    "with open('model_output.json', 'w') as f:\n",
    "    json.dump(model_output_to_dict(encoded, decoded), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "executorch-05-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
