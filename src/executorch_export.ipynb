{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting the model via executorch for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook exports the model to be used in an Android app via executorch runtime\n",
    "\n",
    "Let's say that the root of the android app is called `executorch_neuroswipe_example_1`. So each time you see a root with `executorch_neuroswipe_example_1` in it, it means the root of the Android App you're building\n",
    "\n",
    "I created the app from a blank android project.\n",
    "\n",
    "To use the model in an app:\n",
    "1. Follow [official instructions](https://pytorch.org/executorch/0.4/getting-started-setup.html) for installing the **0.4** release of executorch. Note that I used 0.4 version!\n",
    "2. Follow [official instructions](https://pytorch.org/executorch/0.4/demo-apps-android.html) for buildig . Note that you only need instruction related to XNNPACK and don't need anything related to Qualcomm Hexagon NPU.\n",
    "    * So basically, You do the `Build the CMake target for the library with XNNPACK backend:` and `Build the Android extension:` under the `XNNPACK` header\n",
    "3. Create a folder `executorch_neuroswipe_example_1/app/src/main/jniLibs/arm64-v8a` and copy libexecutorch_jni.so from `executorch/cmake-android-out/extension/android/libexecutorch_jni.so` there. Then rename `libexecutorch_jni.so` to `libexecutorch.so`. Thus the path to the library is: `executorch_neuroswipe_example_1/app/src/main/jniLibs/arm64-v8a/libexecutorch_jni.so`\n",
    "4. Put the model in `.pte` format exported in this notebook to the folder `executorch_neuroswipe_example_1/app/src/main/assets`\n",
    "5. Add to the end of `executorch_neuroswipe_example_1/settings.gradle.kts` file a line  `includeBuild(\"../../Documents/executorch_test_examples/executorch_getting_started_example/executorch/extension/android\")`\n",
    "\n",
    "6. Add Following to android manifest:\n",
    "``` xml\n",
    "<uses-native-library\n",
    "   android:name=\"libexecutorchdemo.so\"\n",
    "   android:required=\"false\" />\n",
    "\n",
    "\n",
    "<uses-native-library android:name=\"libcdsprpc.so\"\n",
    "   android:required=\"false\"/>\n",
    "```\n",
    "\n",
    "7. Add following to `build.gradle.kts`:\n",
    "``` \n",
    "implementation(\"com.facebook.soloader:soloader:0.10.5\")\n",
    "implementation(\"com.facebook.fbjni:fbjni:0.5.1\")\n",
    "implementation(\"org.pytorch.executorch:executorch\") {\n",
    "   exclude(\"com.facebook.fbjni\", \"fbjni-java-only\")\n",
    "}\n",
    "```\n",
    "8. Copied BUCK file from the official example app (though I'm not sure that the BUCK file is needed)\n",
    "9. Maybe I did some more steps that I forgot about ðŸ™ƒ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "1. remove dropout from exported programs\n",
    "2. Ð²ÐµÑ€Ð¾ÑÑ‚Ð½Ð¾ Ð½ÑƒÐ¶Ð½Ð¾ ÑÐ´ÐµÐ»Ð°Ñ‚ÑŒ Ð¼Ð¾Ð´ÑƒÐ»Ð¸ FullEncoder \n",
    "3. Ð¿Ñ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ, Ð¸Ð³Ð½Ð¾Ñ€Ð¸Ñ€ÑƒÑŽÑ‚ÑÑ Ð¿Ð¾Ð»Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð½Ðµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑŽÑ‚ÑÑ Ð² forward (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, ÐµÑÑ‚ÑŒ Ð»Ð¸ Ñ€Ð°Ð·Ð½Ð¸Ñ†Ð° Ð¼ÐµÐ¶Ð´Ñƒ Ñ‚ÐµÐºÑƒÑ‰ÐµÐ¹ Ð¸Ð½Ð¼Ð¿Ð»ÐµÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸ÐµÐ¹ `Decode` Ð¸ Ð¸Ð¼Ð¿Ð»ÐµÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸ÐµÐ¹, Ð³Ð´Ðµ Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ñ…Ñ€Ð°Ð½Ð¸Ñ‚ÑÑ ÐºÐ°Ðº ÑƒÐ´Ð¸Ð½ÑÑ‚Ð²ÐµÐ½Ð½Ð¾Ðµ Ð¿Ð¾Ð»Ðµ Decode) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/proshian/Documents/executorch_test_examples/executorch_examples_venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Union, Tuple\n",
    "import os\n",
    "import array\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.export import export, ExportedProgram, Dim\n",
    "from executorch.exir import EdgeProgramManager, to_edge, to_edge_transform_and_lower\n",
    "from executorch.exir.backend.backend_api import LoweredBackendModule, to_backend\n",
    "from executorch.backends.xnnpack.partition.xnnpack_partitioner import XnnpackPartitioner\n",
    "\n",
    "from model import MODEL_GETTERS_DICT, EncoderDecoderTransformerLike\n",
    "from feature_extractors import get_val_transform\n",
    "from ns_tokenizers import CharLevelTokenizerv2, KeyboardTokenizerv1\n",
    "from word_generators_v2 import BeamGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_prefix(s: str, prefix: str) -> str:\n",
    "    if s.startswith(prefix):\n",
    "        s = s[len(prefix):]\n",
    "    return s\n",
    "\n",
    "\n",
    "def get_state_dict_from_checkpoint(ckpt: dict) -> Dict[str, torch.Tensor]:\n",
    "    return {remove_prefix(k, 'model.'): v for k, v in ckpt['state_dict'].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMAND LINE ARGUMENTS EMULATION\n",
    "\n",
    "MODEL_NAME = 'v3_nearest_and_traj_transformer_bigger'\n",
    "CHECKPOINT_ROOT_PATH = '../../../checkpoints_for_executorch/my_nearest_features/'\n",
    "CHECKPOINT_PATH = os.path.join(CHECKPOINT_ROOT_PATH, 'v3_nearest_and_traj_transformer_bigger-default--epoch=73-val_loss=0.444-val_word_level_accuracy=0.872.ckpt')\n",
    "TRANSFORM_NAME =  \"traj_feats_and_nearest_key\"\n",
    "\n",
    "DATA_ROOT = '../data/data_separated_grid'\n",
    "\n",
    "GRIDNAME_TO_GRID_PATH = os.path.join(DATA_ROOT, \"gridname_to_grid.json\")\n",
    "voc_path=os.path.join(DATA_ROOT, \"voc.txt\")\n",
    "char_tokenizer = CharLevelTokenizerv2(voc_path)\n",
    "kb_tokenizer = KeyboardTokenizerv1()\n",
    "\n",
    "USE_TIME = False\n",
    "USE_VELOCITY = True\n",
    "USE_ACCELERATION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/proshian/Documents/executorch_test_examples/executorch_examples_venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the trained model\n",
    "\n",
    "state_dict = get_state_dict_from_checkpoint(\n",
    "    torch.load(CHECKPOINT_PATH, map_location='cpu', weights_only=True))\n",
    "\n",
    "model: EncoderDecoderTransformerLike = MODEL_GETTERS_DICT[MODEL_NAME]().eval()\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing model input \n",
    "1. encoder_in\n",
    "2. decoder_in, decoder_out_target, encoded_swipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prepare_encoder_input(encoder_in: Union[Tensor, Tuple[Tensor, Tensor]], \n",
    "                           device: str, batch_first: bool\n",
    "                           ) -> Tuple[Tensor, Tensor]:\n",
    "    is_tensor = None\n",
    "    if isinstance(encoder_in, Tensor):\n",
    "        is_tensor = True\n",
    "        encoder_in = [encoder_in]\n",
    "    else:\n",
    "        is_tensor = False\n",
    "\n",
    "    encoder_in = [el.unsqueeze(0) if batch_first else el.unsqueeze(1) for el in encoder_in]\n",
    "    encoder_in = [el.to(device) for el in encoder_in]\n",
    "\n",
    "    return encoder_in[0] if is_tensor else encoder_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATASET_ITEM_EXAMPLE = (\n",
    "    array.array('h', [567, 567, 507, 424, 380, 348, 337, 332, 330, 329, 327, 326, 326]),\n",
    "    array.array('h', [66, 66, 101, 161, 196, 230, 240, 245, 247, 249, 251, 251, 251]),\n",
    "    array.array('h', [0, 3, 24, 52, 75, 90, 106, 129, 145, 161, 177, 195, 209]),\n",
    "    'default',\n",
    "    'Ð½Ð°')\n",
    "\n",
    "GRIDNAME_TO_GRID_PATH = '../data/data_separated_grid/gridname_to_grid.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulating out-of-bounds coordinates...\n",
      "augmenting gname_to_out_of_bounds\n"
     ]
    }
   ],
   "source": [
    "transform = get_val_transform(\n",
    "    gridname_to_grid_path=GRIDNAME_TO_GRID_PATH,\n",
    "    grid_names=['default'],\n",
    "    transform_name=TRANSFORM_NAME,\n",
    "    char_tokenizer=char_tokenizer,\n",
    "    uniform_noise_range=0,\n",
    "    include_time=USE_TIME,\n",
    "    include_velocities=USE_VELOCITY,\n",
    "    include_accelerations=USE_ACCELERATION,\n",
    "    dist_weights_func=None,  # Fill if weighted version is used\n",
    "    ds_paths_list=[],\n",
    ")\n",
    "\n",
    "\n",
    "(encoder_in, decoder_in), decoder_out_target = transform(RAW_DATASET_ITEM_EXAMPLE)\n",
    "encoder_in = _prepare_encoder_input(encoder_in, 'cpu', batch_first=False)\n",
    "if isinstance(encoder_in, list):\n",
    "    encoder_in = tuple(encoder_in)\n",
    "decoder_in = decoder_in.unsqueeze(1).to(torch.int32)\n",
    "\n",
    "encoded = model.encode(encoder_in, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative: Manual data creation\n",
    "In this case all inputs are torch.ones. We can allso generate random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get data manually\n",
    "# SWIPE_LENGTH = 13\n",
    "# BATCH_SIZE = 1\n",
    "# NUM_TRAJ_FEATS = 6\n",
    "# OUT_SEQ_LEN = 3\n",
    "\n",
    "# sample_kb_key_ids = torch.ones((SWIPE_LENGTH, BATCH_SIZE), dtype=torch.int32)\n",
    "# sample_traj_feats = torch.ones((SWIPE_LENGTH, BATCH_SIZE, NUM_TRAJ_FEATS), dtype=torch.float32)\n",
    "# encoder_in = (sample_traj_feats, sample_kb_key_ids)\n",
    "# decoder_in = torch.ones((OUT_SEQ_LEN, BATCH_SIZE), dtype=torch.int32)\n",
    "# encoded = model.encode(\n",
    "#     encoder_in, \n",
    "#     None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encode(torch.nn.Module):\n",
    "    def __init__(self, model) -> None:\n",
    "        super().__init__()\n",
    "        self.enc_in_emb_model = model.enc_in_emb_model\n",
    "        self.encoder = model.encoder\n",
    "\n",
    "    def forward(self, encoder_in):\n",
    "        x = self.enc_in_emb_model(encoder_in)\n",
    "        result = self.encoder(x, src_key_padding_mask = None)\n",
    "        return result\n",
    "\n",
    "\n",
    "class Decode(torch.nn.Module):\n",
    "    def __init__(self, model) -> None:\n",
    "        super().__init__()\n",
    "        self.dec_in_emb_model = model.dec_in_emb_model\n",
    "        self.decoder = model.decoder\n",
    "        self._get_mask = model._get_mask\n",
    "        self.out = model.out\n",
    "\n",
    "    def forward(self, decoder_in, x_encoded):\n",
    "        y = self.dec_in_emb_model(decoder_in)\n",
    "        tgt_mask = self._get_mask(y.size(0))  # = self.causal_mask[y.size(0):, y.size(0):]\n",
    "        dec_out = self.decoder(\n",
    "            y, x_encoded, tgt_mask=tgt_mask, \n",
    "            memory_key_padding_mask=None, \n",
    "            tgt_key_padding_mask=None,\n",
    "            tgt_is_causal=True)\n",
    "        return self.out(dec_out)\n",
    "    \n",
    "\n",
    "MAX_SWIPE_LEN = 299\n",
    "MAX_WORD_LEN = 35\n",
    "dim_swipe_seq = Dim(\"dim_swipe_seq\", min=1, max=MAX_SWIPE_LEN)\n",
    "dim_char_seq = Dim(\"dim_char_seq\", min=1, max=MAX_WORD_LEN)\n",
    "\n",
    "encoder_dynamic_shapes = {\"encoder_in\": ({0: dim_swipe_seq}, {0: dim_swipe_seq})}\n",
    "decoder_dynamic_shapes = {\n",
    "    \"x_encoded\": {0: dim_swipe_seq},\n",
    "    \"decoder_in\": {0: dim_char_seq}\n",
    "}\n",
    "\n",
    "aten_encode: ExportedProgram = export(Encode(model).eval(), (encoder_in,), dynamic_shapes=encoder_dynamic_shapes)\n",
    "aten_decode: ExportedProgram = export(Decode(model).eval(), (decoder_in, encoded), dynamic_shapes=decoder_dynamic_shapes)\n",
    "\n",
    "edge_xnnpack: EdgeProgramManager = to_edge_transform_and_lower(\n",
    "    {\"encode\": aten_encode, \"decode\": aten_decode},\n",
    "    partitioner=[XnnpackPartitioner()],\n",
    ")\n",
    "\n",
    "exec_prog_xnnpack = edge_xnnpack.to_executorch()\n",
    "\n",
    "with open(\"xnnpack_my_nearest_feats.pte\", \"wb\") as file:\n",
    "    exec_prog_xnnpack.write_to_file(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_program = to_edge({\"encode\": aten_encode, \"decode\": aten_decode})\n",
    "\n",
    "executorch_program = edge_program.to_executorch()\n",
    "\n",
    "with open(\"raw_my_nearest_feats.pte\", \"wb\") as file:\n",
    "    file.write(executorch_program.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(aten_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(exec_prog_xnnpack.exported_program('encode'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exec_prog_xnnpack.exported_program('encode').module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's create tests to make sure that the model in the app creates same output as a pytorch version\n",
    "\n",
    "We'll save the transformed validation data to files (encoder_in, decoder_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data manually\n",
    "SWIPE_LENGTH = 13\n",
    "BATCH_SIZE = 1\n",
    "NUM_TRAJ_FEATS = 6\n",
    "OUT_SEQ_LEN = 3\n",
    "\n",
    "sample_kb_key_ids = torch.ones((SWIPE_LENGTH, BATCH_SIZE), dtype=torch.int64)\n",
    "sample_traj_feats = torch.ones((SWIPE_LENGTH, BATCH_SIZE, NUM_TRAJ_FEATS), dtype=torch.float32)\n",
    "encoder_in = (sample_traj_feats, sample_kb_key_ids)\n",
    "decoder_in = torch.ones((OUT_SEQ_LEN, BATCH_SIZE), dtype=torch.int64)\n",
    "\n",
    "encoded = model.encode(\n",
    "    encoder_in, \n",
    "    None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = model.decode(decoder_in, encoded, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_dict(tensor: torch.Tensor) -> dict:\n",
    "    return {\n",
    "        'data': tensor.reshape(-1).tolist(),\n",
    "        'shape': tuple(tensor.shape)\n",
    "    }\n",
    "\n",
    "def model_input_to_dict(encoder_in, decoder_in):\n",
    "    return {\n",
    "        'encoder_in': [tensor_to_dict(encoder_in_i) for encoder_in_i in encoder_in],\n",
    "        'decoder_in': tensor_to_dict(decoder_in)\n",
    "    }\n",
    "\n",
    "def model_output_to_dict(encoder_out, decoder_out):\n",
    "    return {\n",
    "        'encoder_out': tensor_to_dict(encoder_out),\n",
    "        'decoder_out': tensor_to_dict(decoder_out)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_input.json', 'w') as f:\n",
    "    json.dump(model_input_to_dict(encoder_in, decoder_in), f)\n",
    "\n",
    "with open('model_output.json', 'w') as f:\n",
    "    json.dump(model_output_to_dict(encoded, decoded), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "executorch_examples_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
